---
title: "Point Interpolation from FVCOM Mesh"
description: | 
  Processing Regional Timeseries of Surface and Bottom Temperatures
date: "Updated on: `r Sys.Date()`"
format: 
  html:
    code-fold: true
    code-tools: true
    df-print: kable
    self-contained: true
execute: 
  echo: true
  warning: false
  message: false
  fig.align: "center"
  comment: ""
---

```{r}

####. packages. ####
library(gmRi)           # for building file paths to box
library(tidyverse)      # data wrangling and plotting
library(sf)             # spatial data support
library(rnaturalearth)  # shapefiles for country and states
library(fvcom)          # Bigelow package for dealing with FVCOM
library(ncdf4)          # Support for netcdf files


# Set a plotting theme
theme_set(theme_gmri_simple())

# Project paths on box
lob_ecol_path <- cs_path("mills", "Projects/Lobster ECOL")
fvcom_path    <- cs_path("res", "FVCOM/Lobster-ECOL")
poly_paths    <- cs_path("mills", "Projects/Lobster ECOL/Spatial_Defs")

# State and Province Shapefiles for US + Canada
# Downloaded locally as part of the rnaturalearthhires package
new_england <- ne_states("united states of america", returnclass = "sf") %>% 
  filter(postal %in% c("VT", "ME", "RI", "MA", "CT", "NH", "NY", "MD", "VA", "NJ", "DE", "NC", "PA", "WV"))
canada <- ne_states("canada", returnclass = "sf")
```


# Within-Mesh Point-Location Values from FVOM

**About:**

This is a start-to-finish demo of returning data from the FVCOM for point(s) that fall within the FVCOM mesh, but not directly on the nodes.

This workflow applies an interpolation method to retun data drawn from nearby mesh nodes.

The FVCOM data I am using as a starting point was prepared for us by Dr. Siqi Li in Dr. Chen's lab at UMASS Dartmouth. A Key difference between these files (which have a limited number of variables) and a "full" FVCOM file is that there is no dimension for depth. 

This difference means that there is one less dimension when indexing data out of these files with square brackets `[]`. This code will need to be modified slighly to index the proper depth (`siglay`) if using another FVCOM file source.


# FVCOM Temperature Interpolations for Survey Stations Points

One example usage of this workflow is for pairing modeled bottom temperature values with locations from various biological survey programs. These are long-running programs that happen all over the continental shelf, that may not have sensors for bottom temperature or salinity etc. It is a common routine to supplement these point samples with external datasets that are modeled or remotely sensed.

### Load VTS Survey Data

One such biological sampling program is the the ventless trap survey (VTS), which is used to monitor juvenile american lobsters. This is a state-run program, and we have point locations for the Maine and Massachusetts VTS surveys. Data from this survey provides information on the time/place traps were set which we will use in this demo to extract the relevant FVCOM data. 

First thing is to load these datasets and pull out the unique date/time information that is the minimum information we need for the FVCOM extraction. This information can later be joined back in to the full dataset uing the date/time location for each.

```{r}
#| label: load VTS points

# Path to resources
vts_path <- cs_path("mills", "Projects/Lobster/VTS_fromASMFC")

# Maine
load(str_c(vts_path, "VTS_data.Rdata"))

# Mass
load(str_c(vts_path, "VTS_MA_Proc_240201 all data for standardized index.Rdata"))

# Combine the pieces from each state
trips_both <- bind_rows(Trips, mutate(Trips_MA, Fisher = as.character(Fisher)))
trawl_both <- bind_rows(Trawls, Trawls_MA)


# Need Trips (for date) and Trawls, and Sites I think
# there are bugs on these trips, because they have multiple records for stocks: SNE, GOM, NA: 
# trips_both[319,]
# trawl_both[25969,]
# trips_both %>% filter(TripId == "MA_20060605_1")
# trawl_both %>% filter(TripId == "MA_20060605_1")

# Combine them to one dataframe, joining on tripid
vts_both <- inner_join(
    trips_both %>% distinct(TripId, State, Date),
    trawl_both, 
    join_by(TripId))

# Get the unique dates and locations for extracting fvcom with
vts_trips <- vts_both  %>% 
  distinct(TripId, TrawlId, SiteId, Date, Longitude, Latitude) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, remove = F) 

```

### Prepare Directory Links to FVCOM

FVCOM Data was delivered via google drive, and then transferred to Box. There is a file with daily data for each year from 1978-2019.

These files are in this directory:   
`Box/Res_Data/FVCOM/Lobster-ECOL/`

For our workflow: we will be opening files sequentially, clipping data for the relevant areas, and returning a dataframe for that period of time. These then get sandwiched together at the end to make one file.



```{r}
#| label: build paths to fvcom inventory


# Here are the files we have, loop over them later
fvcom_surfbot_files <- setNames(
  list.files(fvcom_path, full.names = T, pattern = ".nc"),
  str_remove(list.files(fvcom_path, full.names = F, pattern = ".nc"), ".nc"))


```


### Loading one FVCOM File to Build Mesh


From the latitude and longitude information in the FVCOM file, we can construct an sf dataframe to represent the mesh spatially on a map. This is helpful for letting us "see" what we're working with.


```{r}
#| label: load one fvcom file



# Test File: GOM3 1978
# Load some daily FVCOM that we downloaded and averaged
fvcom_yrx <- nc_open(fvcom_surfbot_files["gom3_1978"])

# # The variables we have in each file
# names(fvcom_yrx$var)

# Get the mesh itself as a simple feature collection
gom3_mesh <- fvcom::get_mesh_geometry(fvcom_yrx, what = 'lonlat')

```


### Using the FVCOM Mesh to Index

The following map shows the coverage overlap between trawl survey locations and FVCOM GOM3.

```{r}

# Map everything
ggplot() +
  geom_sf(data = gom3_mesh, alpha = 0.2, linewidth = 0.05, color = "gray30") +
  geom_sf(
    data = vts_trips,
    aes(color = "VTS"),
    shape = 3, size = 0.2, alpha = 0.2) +
  geom_sf(data = new_england) +
  geom_sf(data = canada) +
  scale_fill_gmri() +
  coord_sf(xlim = c(-72, -67), ylim = c(41.25, 45.5)) +
  labs(
    title = "Coverage overlap of FVCOM and Sample Points",
    color = "VTS Survey Locations")


```



## Getting Values from Nearby Points

FVCOM (and similar models) store data at either the points where edges meet (nodes), or at the center point of each triangle element (centroid/elem). There is no explicit data in the gaps between these things, so operations that one might use with a raster, like overlaying the grid and extracting a value are not as directly achieved.

To get values for points that fall within the mesh, we need to interpolate values from the FVCOM hindcast data for that point in time using information from the nearest mesh nodes. Possible ways to do this is include using linear interpolation, inverse distance weighting, nearest neighbors, barycentric interpolation etc. 

The following code will step through: 
 1. triangulating point locations from the VTR survey to the three nearest/surrounding nodes   
 2. assigning weights to the three nodes based on distance for linear interpolation (recommended by Chen lab)
 
This step provides an indexing table that can be applied to any FVCOM hindcast file to subset and interpolate variables to these point locations.

Once interpolated values are extracted I will do a check to compare how the FVCOM hindcast data performs relative to the CTD data from the survey.

```{r}
# Function to add the linear interpolation weights based on node coordinates
triangulate_and_weight <- function(pts_sf, fvcom_mesh){

    # Identify the triangles that overlap each point:
    # Use st_join to assign elem, p1, p2, p3 IDs to the points
    pts_assigned <- st_join(
      st_transform(pts_sf, st_crs(fvcom_mesh)),
      gom3_mesh, 
      join = st_within) %>% 
      drop_na(elem)
  

    # Iterate over the rows to add weights:
    pts_weighted <- pts_assigned %>% 
     base::split(., seq_len(nrow(.))) %>%
     purrr::map_dfr(function(pt_assigned){
    
      # Subset the relevant triangle from st_join info
      triangle_match <- fvcom_mesh[pt_assigned$elem,]
      
      # Build matrices for point to interpolate & of surrounding points:
    
      # Matrix for triangle
      # Use the triangles node coordinates from the sf geometries
      # Creates 3x3 matrix: row1 x coords, row 2, y coords, row three rep(1,3)
      node_vertices <- t(st_coordinates(triangle_match[1,])[1:3,1:3])
      
      # Make matrix from the points:
      # creates 3x1 matrix: x, y, 1
      point_coords <- matrix(
        c(st_coordinates(pt_assigned[1,]), 1), 
        nrow = 3)
      
      #### For Linear Interpolation:
      
      # Get inverse of the matrix
      inverse_coordmat <- solve(node_vertices)
      
      # Solve for the weights
      node_wts <- inverse_coordmat %*% point_coords %>%
        t() %>% 
        as.data.frame() %>% 
        setNames(c("p1_wt", "p2_wt", "p3_wt"))
      
      # Return with dataframe
      bind_cols(pt_assigned, node_wts)
    
    
    })
    # End Rowwise
    return(pts_weighted)
}
```


When applied to the VTS point locations, we get a table returned that looks like this (first six rows)

```{r}
#| label: get weights for each point

# Make the vts locations an sf dataframe
vts_pts_sf <- vts_trips %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, remove = F)


# Run for all points:
vts_pts_weighted <- triangulate_and_weight(
  pts_sf = vts_pts_sf, 
  fvcom_mesh = gom3_mesh) %>% 
  st_drop_geometry()

# Display a subset
vts_pts_weighted %>% 
  head() %>% 
  gt::gt() %>% 
  gt::fmt_number(columns = where(is.numeric), decimals = 2) %>% 
  gt::fmt_number(columns = c("elem", "p1", "p2", "p3"), decimals = 0)
```



# Handling the Time Dimension


What I am working towards for this workflow is preparing a dataframe where each row of data contains all the indexing and weighting information needed to interpolate values from the surrounding nodes.

At this point `vts_pts_weighted` contains the indexing information for the nodes `p1`, `p2`, `p3` & the weighting for each of those values: `p1_wt`, `p2_wt`, `p3_wt`. 

From this point, we need to now set up the indexing for the `time` dimension. The two approaches that follow each build off of the steps above.


# Approach 1: Same Day Date Matching

Now that we have the blueprint for how to handle the point locations in lat/lon space, we need to decide how to efficiently handle the time dimension.

The most straightforward approach is to match the date of the vts sample to the same date in the FVCOM data, and get values for the same point in time.


## Get the Proper Time Index for Each File

Each FVCOM dataset we've been sent stores data for one year, waith daily records for the full year. The next chunk of code opens them each up sequentially and builda a table indicating which dates are stored in which file.

We can join this information into our dataframe of vts points to add in details of which fvcom file to open and which date to index out.


```{r}
#| label: build date table from fvcom files

# Get the dates within each file
# This code opens each file in our list, pulls the time values, and closes them
# It stores this information in a big table with three columns: 
# fv_file = name of fvcom file in our list
# fvcom_date = date in "yyyy-mm-dd"
# time_idx = integer index number for that date
fvcom_dates <- map_dfr(
  fvcom_surfbot_files,
   function(x){
     x_fvcom <- nc_open(x)
     timez <- ncvar_get(x_fvcom, "Times")
     nc_close(x_fvcom)
     return(
       data.frame("fvcom_date" = timez) %>% 
         mutate(
           fvcom_date = as.Date(fvcom_date),
           time_idx = row_number()))
   }, .id = "fv_file")
```

The following code performs that join, identifying the correct date timestep indexing information for each poin location and adding that information to the dataframe.

After this is done, we have all the information we need to loop/map through the different years and interpolate values for all the points.

This all sets things up such that we aren't pulling out more data than is absolutely necessary to give us what we need.


```{r}
# If we join our vts survey points to this table,
# we add in the fvcom_file that contains the data for each record

# Join them by date to look at their matches
vts_dates_matched <- vts_pts_weighted %>% 
  mutate(Date = as.Date(Date)) %>% 
  left_join(
    fvcom_dates,
    join_by(Date == fvcom_date))

```



## Same-Date Point Interpolation

The last step in this approach is to take the information we've prepared to pull the data from each FVCOM file and return the result of the linear interpolation.

 
This step can be looped on each date/year to minimize the amount of fvcom file opening/closing. Within each year we need to identify which timestep to extract data at, and then iterate on them.


```{r}


#' Interpolate Values from FVCOM Mesh at Timestep
#' 
#' @description Takes a dataframe row containing node and time indices and interpolation weights and returns that contains time index from which to interpolate.
#' 
#' Data passed to function should contain the following columns:
#' time_idx = integer timestep to use for interpolation
#' p1 = integer index value for node 1 surrounding the interpolation location
#' p2 = integer index value for node 2 surrounding the interpolation location
#' p3 = integer index value for node 3 surrounding the interpolation location
#' p1_wt = linear interpolation weight for p1
#' p2_wt = linear interpolation weight for p1
#' p3_wt = linear interpolation weight for p1
#'
#' @param dated_points_weighted dataframe row containing time index, node index, and node weight information columns
#' @param fvcom_nc FVCOM netcdf file to extract values from
#' @param fvcom_varid = String indicating variable in FVCOM netcdf to interpolate values with
#' @param var_out String to use as variable name in returned dataframe
#'
#' @return
#' @export
#'
#' @examples
interpolate_at_timestep <- function(
    dated_points_weighted, 
    fvcom_nc, 
    fvcom_varid, 
    var_out){
        
    # Get the values of the variable of interest as vector
    node_vals <- ncvar_get(
      nc = fvcom_nc, 
      varid = fvcom_varid, 
      start = c(1, dated_points_weighted[["time_idx"]]),
      count = c(-1, 1))
      
  # Interpolate using the node numbers and weights
  dated_interpolation <- dated_points_weighted %>% 
    mutate(
      {{var_out}} := node_vals[p1] * p1_wt + node_vals[p2] * p2_wt + node_vals[p3] * p3_wt)
  
  return(dated_interpolation)
}



```


## Iterate on Years to Interpolate All Points

Now that we have the indexing information prepared, we can loop over the yearly files and obtain surface and bottom temperatures. 

I'm using yearly timesteps to loop because we have yearly files, this way we only need to open each year once, then slice out values at the corresponding timesteps within them.

```{r}

# Operate over years
vts_fvcom_temps <- vts_dates_matched %>% 
  drop_na(time_idx) %>%  # Skip any records with no matches
  mutate(year = year(Date)) %>% 
  
  # Split into a list based on year to match the file structure
  split(.$year) %>% 
  map_dfr(
    .f = function(samples_year_x){
    # Get the file to open
    nc_name <- samples_year_x[["fv_file"]][[1]][[1]]
    
    # Open the corresponding Netcdf
    fvcom_yr_x <- nc_open(fvcom_surfbot_files[[nc_name]])
    
    # Split the samples by date
    locations_bydate <- samples_year_x %>% 
      base::split(., seq_len(nrow(.))) 
    
    # Iterate on those - do bottom temp and surface temp
    dates_interpolated <- locations_bydate %>%
      map(.f = ~interpolate_at_timestep(
        dated_points_weighted = .x,
        fvcom_nc = fvcom_yr_x,
        fvcom_varid = "surface_t", 
        var_out = "surf_temp_c")) %>% 
      map_dfr(.f = ~interpolate_at_timestep(
        dated_points_weighted = .x,
        fvcom_nc = fvcom_yr_x,
        fvcom_varid = "bottom_t", 
        var_out = "bot_temp_c"))
    
    return(dates_interpolated)
    
  })
 
```


Once this is done, we have a dataset containing the fvcom surface and bottom conditions for the same date. These can be joined back to some/all of the rest of the ventless trap data as desired.

```{r}
# # Rejoining looks like this
# vts_augmented <- left_join(
#   vts_both,          # original data from Maine and Mass 
#   vts_fvcom_temps,   # The data we pulled from FVCOM
#   by = join_by(TripId, Date, TrawlId, SiteId, Latitude, Longitude))



# 
vts_fvcom_temps %>% 
  mutate(month = month(Date, label = T)) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326) %>% 
  ggplot() +
  geom_sf(aes(color = bot_temp_c), size = .3) +
  geom_sf(data = new_england) +
  geom_sf(data = canada) +
  facet_wrap(~month) +
  scale_color_distiller(palette = "RdBu") +
  coord_sf(xlim = c(-72, -67), ylim = c(41.25, 45.5)) +
  labs(
    title = "Daily Bottom Temperature from FVCOM",
    subtitle = "VTS Survey Locations - Matched to same-day data",
    color = "Bottom Temperature (C)")

```

For details on doing this for the different surveys see: `Survey_Locations_FVCOM_Temp_Interpolations.qmd`


# Approach 2: Seasonal Averages (Instead of Same-Day)

I mentioned above that sometimes it may be of interest to get conditions over some period of time rather than the exact date. As a demonstration below, I will return the average conditions from spring (March-May) and Summer (June-August).

The main difference here than what we just did is that we are subsetting a vector of temperatures (a value for each day) for each surrounding node, rather than a single value.

This means we need to change the code a touch:


```{r}
# Put the seasonal flags into the date key

# If we have different groups set them up this way to make life easier
# Start mm-dd & end mm-dd, as a named list
season_key <- list(
  "Spring" = c("03-01", "05-31"),
  "Summer" = c("06-01", "08-31"),
  "Fall" = c("09-01", "11-30"))




# This next step will take whatever is in that list,
# And filter out the date and indices for each fvcom file that are relevant
season_index <- season_key %>% 
  map_dfr(function(x){
    
    # Each year, filter within those dates
    date_range <- fvcom_dates %>% 
      mutate(fvyear = year(fvcom_date)) %>% 
      filter(
        between(
          fvcom_date, 
          as.Date(str_c(fvyear, "-", x[[1]])),
          as.Date(str_c(fvyear, "-", x[[2]])))) %>% 
      # Then summarise by year to get the min/max
      group_by(fvyear) %>% 
      summarise(
        start_idx = min(time_idx, na.rm = T),
        end_idx = max(time_idx, na.rm = T),
        .groups = "drop") %>% 
      mutate(idx_count = end_idx - start_idx)
    
    # Combine to one table
    }, .id = "season")
```

### Join Season Indexing

Similar to the same-day indexing approach above, we want to add the time-period indexing information to `vts_pts_weighted` so that each row in the dataframe can be fed to some function and return the correct interpolated value.



```{r}
# We could reduce that to just the min/max of each month for a start/stop

# Join the seasonal indexing data by year to the vts stations
vts_pts_weighted[1,]
season_index[29,]

# Join the season indexing to the triangulated stuff
vts_season_matched <- vts_pts_weighted %>% 
  mutate(
    Date = as.Date(Date),
    year = year(Date)) %>% 
  # This join adds a row for each season
  full_join(
    season_index,
    join_by(year == fvyear),
    relationship = "many-to-many")
```



```{r}
# Now we need a function to pull dates over those ranges

# This is the idea:
# use start and end indexes for time, get average, then weight the thre points for interpolation

# Take the first row as a test for pulling seasonal data out
test_range <- vts_season_matched[1, ] # row, column dataframe indexing


# This returns days for one point, within the season we're indexing
test_return <- ncvar_get(
  nc = fvcom_yrx,
  varid = "surface_t",
  start = c(test_range[["p1"]], test_range[["start_idx"]]),
  count = c(1, test_range[["idx_count"]]))

# we average that (to return one mean for the period/season)
# weight it by the distance weight: p1_wt
# repeat that for points p2 and p3 respectively, then combine

```


Claire, my friend. The function below takes data from all rows, and applys the interpolation step.

```{r}
# Function to strip relevant FVCOM data for interpolating seasonal averages for point locations
# Takes an input dataframe that contains: 
# indexing information for three nearest points (to interpolate with)
# distance weights for linear interpolation
# time period start and count index information
interpolate_over_timerange <- function(
    pts_wts_ranges, fvcom_nc, fvcom_varid, var_out){
  
  
  # We're gonna need to go rowwise over each location
  # Otherwise its going to be too much data to load
  
  # This function will be applied to each row:
  period_index_to_mean <- function(p1, p1_wt, p2, p2_wt, p3, p3_wt, start_idx, idx_count, ...){
    
    # Get the values of the variable of interest as vector
    # Pull the time period, get mean, weight
    p1_mu  <- ncvar_get(
      nc = fvcom_nc, 
      varid = fvcom_varid, 
      start = c(p1, start_idx),
      count = c(1, idx_count)) %>% 
      mean() * p1_wt
    p2_mu  <- ncvar_get(
        nc = fvcom_nc, 
        varid = fvcom_varid, 
        start = c(p2, start_idx),
        count = c(1, idx_count)) %>% 
        mean() * p2_wt
    p3_mu  <- ncvar_get(
          nc = fvcom_nc, 
          varid = fvcom_varid, 
          start = c(p3, start_idx),
          count = c(1, idx_count)) %>% 
          mean() * p3_wt
    # Then we can averaage the three.
    period_interpolated <- mean(c(p1_mu, p2_mu, p3_mu))
    }

  
    # Perform the rowwise operation and return values to original df
    # Using user-supplied column name
    pts_wts_ranges %>% 
      mutate({{var_out}} := pmap_dbl(
        # Use list() here to pull out the proper colummns
        .l = list(p1, p1_wt, p2, p2_wt, p3, p3_wt, start_idx, idx_count),
        .f = period_index_to_mean))
  
}
```

### Testing the new function:

The following code should extract the seasonal

```{r}

# Need to change the code to use the right year, not fvcom_yrx
nc_close(fvcom_yrx)

# Run them for one year
test_year <- "gom3_2006"
fvcom_yrx <- nc_open(fvcom_surfbot_files[test_year])

# Get seasonal temperatures for one year
vts_seasonal_temps <- vts_season_matched %>% 
  # Why are there missing element overlaps?, need to clean up
  drop_na(start_idx, elem, p1, p2, p3) %>% 
  filter(year == 2006) %>% 
  # Interpolate surface temperature
  interpolate_over_timerange(
    .,
    fvcom_nc = fvcom_yrx, 
    fvcom_varid = "surface_t", 
    var_out = "surf_temp_c") %>% 
  # Repeat if we want bottom temperature or another variable
  interpolate_over_timerange(
    pts_wts_ranges = .,
    fvcom_nc = fvcom_yrx, 
    fvcom_varid = "bottom_t", 
    var_out = "bot_temp_c")


# They are different right: yes
# vts_seasonal_temps %>%
#   ggplot(aes(surf_temp_c, bot_temp_c)) +
#   geom_point() +
#   geom_abline(slope = 1, intercept = 0)



# Map the bottom temperature
vts_seasonal_temps %>% 
  pivot_longer(
    cols = ends_with("temp_c"), 
    names_to = "temp_var", 
    values_to = "temp_c") %>% 
  mutate(season = factor(season, levels = c("Spring", "Summer", "Fall"))) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326) %>% 
  ggplot() +
  geom_sf(data = new_england) +
  geom_sf(data = canada) +
  geom_sf(aes(color = temp_c), size = .3) +
  facet_grid(temp_var~season) +
  scale_color_distiller(palette = "RdBu") +
  coord_sf(xlim = c(-72, -67), ylim = c(41.25, 45.5)) +
  labs(
    title = str_c("Seasonal Temperature from FVCOM: ", test_year),
    subtitle = "VTS Survey Locations - Interpolated Seasonal Averages",
    color = "Temperature (C)")


```


### Interpolating Seasonal Temperatures for each year

The last layer of this onion is looping over the yearly files such that we get the seasonal average for the right year:

```{r}

# Map over each year to open the correct netcdf for the relevant points
vts_seasonal_temp_interpolations <- vts_season_matched %>% 
  # Why are there missing element overlaps?, need to clean up
  drop_na(start_idx, elem, p1, p2, p3) %>% 
  # Break into yearly datasets b/c fvcom is yearly
  split(.$year) %>% 
  imap_dfr(function(indexing_data, year_id){
    
    
    # Run them for one year
    year_fname <- str_c("gom3_", year_id)
    fvcom_yrx <- nc_open(fvcom_surfbot_files[year_fname])
    
    # Get seasonal temperatures for one year
    interpolated_seasonal_temps <- indexing_data %>% 
      
      # Interpolate surface temperature
      interpolate_over_timerange(
      .,
      fvcom_nc = fvcom_yrx, 
      fvcom_varid = "surface_t", 
      var_out = "surf_temp_c") %>% 
      # Repeat if we want bottom temperature or another variable
      interpolate_over_timerange(
        pts_wts_ranges = .,
        fvcom_nc = fvcom_yrx, 
        fvcom_varid = "bottom_t", 
        var_out = "bot_temp_c")
    
    # Return that dataframe
    nc_close(fvcom_yrx)
    return(interpolated_seasonal_temps)
    
    
  }, .id = "year")



```

Lets, take a look. As a quick gut check things I wanted to see here are: points change across years (verifies that the split is working), that the seasonal temperatures are consistent i.e. summer should be hot (verifies that the time-period indexing is working), & there should be changes moving forward in time (verifies that the netcdf for each year is changing).

All those things look good below.

```{r}
# Map the bottom temperature
vts_seasonal_temp_interpolations %>% 
  filter(year %in% c(2008:2012)) %>% 
  mutate(season = factor(season, levels = c("Spring", "Summer", "Fall"))) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326) %>% 
  ggplot() +
  geom_sf(data = new_england) +
  geom_sf(data = canada) +
  geom_sf(aes(color = bot_temp_c), size = .3) +
  facet_grid(season~ year) +
  scale_color_distiller(palette = "RdBu", 
                        labels = scales::label_number(suffix = "\u00b0C")) +
  coord_sf(xlim = c(-72, -67), ylim = c(41.25, 45.5)) +
  labs(
    title = "Seasonal Temperature from FVCOM",
    subtitle = "VTS Survey Locations - Interpolated Seasonal Averages",
    color = "Temperature (C)")
```


# Last Minute Hail Mary

Data for same-day temperatures for different survey datasets were interpolated in `Survey_Locations_FVCOM_Temp_Interpolations.qmd`

The processing of *Seasonal Averages* was coded up here, and needs to be done for these datasets.

Pull seasonal averages for the following datasets:

 1. NEFSC trawl
 2. State trawls: ME/NH, MA, RI, NEAMAP, NJ, and ASMFC from Jef kipp: 
    Box\Mills Lab\Projects\Lobster ECOL\lobster_data\state_trawl   
 4. VTS: ME, MA, and I think a joint file with all the states:    
    Claire\Box\Mills Lab\Projects\Lobster ECOL\lobster_data\vts   
    
    
This is basically the abbreviated version of Approach 2: done for these datasets, and then saved.
 
 
### Load NEFSC Trawl Data

The next chunk of code will read the federal trawl data from box, and pull all distinct stations as well as the ctd bottom temperature reading for comparison.

```{r}
# Load the trawl data
trawl_path <- cs_path("res", "nmfs_trawl/SURVDAT_current")
trawl_raw <- read_rds(str_c(trawl_path, "survdat_lw.rds"))

# tidy it a little
trawl_dat <- gmRi::gmri_survdat_prep(
  survdat = trawl_raw$survdat, 
  survdat_source = "most recent", 
  box_location = "cloudstorage")

# Get distinct time/date/tow/locations
nmfs_trawl <- trawl_dat %>% 
  distinct(cruise6, station, stratum, tow, svvessel, est_towdate, est_year, season, lat = decdeg_beglat, lon = decdeg_beglon, bottemp_ctd = bottemp) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326, remove = F) %>% 
  st_transform(st_crs(gom3_mesh))

```

### Load State Trawl Data

We also have stat trawl survey stations from Jeff Kipp. This includes maine/new hampshire trawl as well as rhode island. This next chunk of code will load them in, and prepare them for overlap/intersection with the fvcom mesh.

```{r}
#| label: load state trawl surveys
#| eval: true

# # Path(s) to state trawl survey data
asmfc_path <- cs_path("mills", "Projects/Lobster/Trawl_fromASMFC")

# # load the different state survey datasets - ASMFC editions
# load(str_c(asmfc_path, "MassDMF_Lobster_StationsNLengths_1979_2023_230718.Rdata"))
# load(str_c(asmfc_path, "MENH Trawl.Rdata"))
load(str_c(asmfc_path, "RI_LOBSTER_TRAWL_012324.Rdata"))
ri_trawl <- RI_Stations %>% 
  distinct(TrawlIdentity, Season, TowNumb, StratumCode, Year, Month, Day, lat = LAT, lon = LON)

# Convert to SF for overlaying on mesh
ri_trawl <- ri_trawl %>% 
  drop_na(lon, lat) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326, remove = F) %>% 
  st_transform(st_crs(gom3_mesh))


# Maine NH Trawl Survey
menh_path <- cs_path("res", "Maine_NH_Trawl/data_2024")
menh_trawl <- read_csv(str_c(menh_path, "MaineDMR_Trawl_Survey_Tow_Data_2024-05-17.csv"))  %>% 
  mutate(
    lon = Start_Longitude,
    lat = Start_Latitude) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326, remove = F) %>% 
  st_transform(st_crs(gom3_mesh))

# Mass Trawl
mass_path  <- cs_path("res", "MA_Trawl/Pull_20240716/Manipulated")
mass_trawl <- read_csv(str_c(mass_path, "MADMF_SVSTA_SW_2024.csv")) %>% 
  mutate(
    lon = `Start lon`,
    lat = `Start lat`) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326, remove = F) %>% 
  st_transform(st_crs(gom3_mesh))


```


### Triangulate State/Federl Survey Locations

This next chunk of code takes each of these survey programs and locates the mesh element and surrounding nodes for each of the points



```{r}
# 2. overlay the points to get the node ids
# Also applies the weights for linear interpolation

# Federal Survey
nmfs_pts_weighted <- triangulate_and_weight(
  pts_sf = nmfs_trawl, 
  fvcom_mesh = gom3_mesh) %>% 
  st_drop_geometry()


# Maine + NH
menh_pts_weighted <- triangulate_and_weight(
  pts_sf = menh_trawl, 
  fvcom_mesh = gom3_mesh) %>% 
  st_drop_geometry()

# Mass
mass_pts_weighted <- triangulate_and_weight(
  pts_sf = mass_trawl, 
  fvcom_mesh = gom3_mesh) %>% 
  st_drop_geometry()

# Rhode Island
ri_pts_weighted <- triangulate_and_weight(
  pts_sf = ri_trawl, 
  fvcom_mesh = gom3_mesh) %>% 
  st_drop_geometry()


```


### Supplement Season Indexing

As we did above, we need to add the indexing information to pull time periods associated with the different seasons.

```{r}
# 1. Join them by their date to match them to FVCOM file names and their time index

# nmfs
nmfs_season_matched <- nmfs_pts_weighted %>% 
  full_join(
    season_index,
    join_by(est_year == fvyear),
    relationship = "many-to-many")

# Maine + NH
menh_season_matched <- menh_pts_weighted %>% 
  full_join(
    season_index,
    join_by(Year == fvyear),
    relationship = "many-to-many")

# Mass
mass_season_matched <- mass_pts_weighted  %>% 
  full_join(
    season_index,
    join_by(Year == fvyear),
    relationship = "many-to-many")

# Rhode Island
ri_season_matched <- ri_pts_weighted %>% 
  full_join(
    season_index,
    join_by(Year == fvyear),
    relationship = "many-to-many")
```



### Interpolate Seasonal Means


```{r}
#| label: nmfs seasonal processing
#| eval: false

# Map over each year to open the correct netcdf for the relevant points
nmfs_seasonal_interpolations <- nmfs_season_matched %>% 
  # Why are there missing element overlaps?, need to clean up
  drop_na(start_idx, elem, p1, p2, p3) %>% 
  # Break into yearly datasets b/c fvcom is yearly
  split(.$est_year) %>% 
  imap_dfr(function(indexing_data, year_id){
    
    
    # Run them for one year
    year_fname <- str_c("gom3_", year_id)
    fvcom_yrx <- nc_open(fvcom_surfbot_files[year_fname])
    
    # Get seasonal temperatures for one year
    interpolated_seasonal_temps <- indexing_data %>% 
      
      # Interpolate surface temperature
      interpolate_over_timerange(
      .,
      fvcom_nc = fvcom_yrx, 
      fvcom_varid = "surface_t", 
      var_out = "surf_temp_c") %>% 
      # Repeat if we want bottom temperature or another variable
      interpolate_over_timerange(
        pts_wts_ranges = .,
        fvcom_nc = fvcom_yrx, 
        fvcom_varid = "bottom_t", 
        var_out = "bot_temp_c")
    
    # Return that dataframe
    nc_close(fvcom_yrx)
    return(interpolated_seasonal_temps)
    
    
  }, .id = "year")



```



```{r}
#| label: menh seasonal processing
#| eval: false

# Map over each year to open the correct netcdf for the relevant points
menh_seasonal_interpolations <- menh_season_matched %>% 
  # Why are there missing element overlaps?, need to clean up
  drop_na(start_idx, elem, p1, p2, p3) %>% 
  # Break into yearly datasets b/c fvcom is yearly
  split(.$Year) %>% 
  imap_dfr(function(indexing_data, year_id){
    
    
    # Run them for one year
    year_fname <- str_c("gom3_", year_id)
    fvcom_yrx <- nc_open(fvcom_surfbot_files[year_fname])
    
    # Get seasonal temperatures for one year
    interpolated_seasonal_temps <- indexing_data %>% 
      
      # Interpolate surface temperature
      interpolate_over_timerange(
      .,
      fvcom_nc = fvcom_yrx, 
      fvcom_varid = "surface_t", 
      var_out = "surf_temp_c") %>% 
      # Repeat if we want bottom temperature or another variable
      interpolate_over_timerange(
        pts_wts_ranges = .,
        fvcom_nc = fvcom_yrx, 
        fvcom_varid = "bottom_t", 
        var_out = "bot_temp_c")
    
    # Return that dataframe
    nc_close(fvcom_yrx)
    return(interpolated_seasonal_temps)
    
    
  }, .id = "year")



```



```{r}
#| label: mass seasonal processing
#| eval: false

# Map over each year to open the correct netcdf for the relevant points
mass_seasonal_interpolations <- mass_season_matched %>% 
  # Why are there missing element overlaps?, need to clean up
  drop_na(start_idx, elem, p1, p2, p3) %>% 
  # Break into yearly datasets b/c fvcom is yearly
  split(.$Year) %>% 
  imap_dfr(function(indexing_data, year_id){
    
    
    # Run them for one year
    year_fname <- str_c("gom3_", year_id)
    fvcom_yrx <- nc_open(fvcom_surfbot_files[year_fname])
    
    # Get seasonal temperatures for one year
    interpolated_seasonal_temps <- indexing_data %>% 
      
      # Interpolate surface temperature
      interpolate_over_timerange(
      .,
      fvcom_nc = fvcom_yrx, 
      fvcom_varid = "surface_t", 
      var_out = "surf_temp_c") %>% 
      # Repeat if we want bottom temperature or another variable
      interpolate_over_timerange(
        pts_wts_ranges = .,
        fvcom_nc = fvcom_yrx, 
        fvcom_varid = "bottom_t", 
        var_out = "bot_temp_c")
    
    # Return that dataframe
    nc_close(fvcom_yrx)
    return(interpolated_seasonal_temps)
    
    
  }, .id = "year")



```


```{r}
#| label: ri seasonal processing
#| eval: false

# Map over each year to open the correct netcdf for the relevant points
ri_seasonal_interpolations <- ri_season_matched %>% 
  # Why are there missing element overlaps?, need to clean up
  drop_na(start_idx, elem, p1, p2, p3) %>% 
  # Break into yearly datasets b/c fvcom is yearly
  split(.$Year) %>% 
  imap_dfr(function(indexing_data, year_id){
    
    
    # Run them for one year
    year_fname <- str_c("gom3_", year_id)
    fvcom_yrx <- nc_open(fvcom_surfbot_files[year_fname])
    
    # Get seasonal temperatures for one year
    interpolated_seasonal_temps <- indexing_data %>% 
      
      # Interpolate surface temperature
      interpolate_over_timerange(
      .,
      fvcom_nc = fvcom_yrx, 
      fvcom_varid = "surface_t", 
      var_out = "surf_temp_c") %>% 
      # Repeat if we want bottom temperature or another variable
      interpolate_over_timerange(
        pts_wts_ranges = .,
        fvcom_nc = fvcom_yrx, 
        fvcom_varid = "bottom_t", 
        var_out = "bot_temp_c")
    
    # Return that dataframe
    nc_close(fvcom_yrx)
    return(interpolated_seasonal_temps)
    
    
  }, .id = "year")



```

### Exporting

```{r}
#| eval: false

# # # Save VTS Bottom temperatures

# # save to project path - Saved here too
# fvcom_processed_path <- cs_path("mills", "Projects/Lobster ECOL/FVCOM_processed/point_location_temperatures")
# save to project path
fvcom_processed_path <- cs_path("mills", "Projects/Lobster ECOL/Regime shift analyses/Point_Location_FVCOM_BT")

# Remove all the point 
remove_fvcom_meta <- function(df){
  df %>% 
    select(-c(p1, p2, p3, p1_wt, p2_wt, p3_wt, start_idx, end_idx, idx_count, elem)) %>% 
    rename(
      fvcom_surf_temp_c = surf_temp_c, 
      fvcom_bot_temp_c = bot_temp_c)
}

# Save all of them out to box
write_csv(
  x = remove_fvcom_meta(nmfs_seasonal_interpolations),
  file = str_c(fvcom_processed_path, "NEFSCtrawl_seasonal_fvcom_temps.csv"))
write_csv(
  x = remove_fvcom_meta(vts_seasonal_temp_interpolations),
  file = str_c(fvcom_processed_path, "VTS_seasonal_fvcom_temps.csv"))
write_csv(
  x = remove_fvcom_meta(menh_seasonal_interpolations),
  file = str_c(fvcom_processed_path, "MENH_seasonal_fvcom_temps.csv"))
write_csv(
  x = remove_fvcom_meta(mass_seasonal_interpolations),
  file = str_c(fvcom_processed_path, "MA_seasonal_fvcom_temps.csv"))
write_csv(
  x = remove_fvcom_meta(ri_seasonal_interpolations),
  file = str_c(fvcom_processed_path, "RI_seasonal_fvcom_temps.csv"))
```

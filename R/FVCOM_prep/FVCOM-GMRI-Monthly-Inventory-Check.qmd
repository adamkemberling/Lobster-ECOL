---
title: "FVCOM Monthly Inventory Check"
description: | 
  Documenting GMRI's FVCOM Inventory on Box
date: "Updated on: `r Sys.Date()`"
format: 
  html:
    code-fold: true
    code-tools: true
    df-print: kable
    self-contained: true
editor: visual
execute: 
  echo: true
  warning: false
  message: false
  fig.height: 6
  fig.width: 8
  fig.align: "center"
  comment: ""
---

## GMRI-Box FVCOM Monthly Inventory

As part of a previous project, the FVCOM GOM3 hindcast data was downloaded for local (cloud) storage. Data was re-saved as monthly means.

This quarto doc will serve as documentation of what data was downloaded, the time/space extent, and which variables were stored.


A previous team member (Matt Dzaugis) was responsible for accessing and storing the data, and we are grateful for his time/effort in doing so.




```{r}
library(raster)
library(sf) 
library(fvcom) 
library(ncdf4) 
library(tidyverse)
library(gmRi)
library(patchwork)

conflicted::conflict_prefer("select", "dplyr")

proj_path <- cs_path("mills", "Projects/Lobster ECOL")
fvcom_out <- str_c(proj_path, "FVCOM_support/")
```

### Accessing Box Inventory

FVCOM is stored with other research community assets in the `RES_Data/` directory.

The GOM3 Monthly Means can be found in `RES_Data/FVCOM/FVCOM_gom3_mon_means` with files labeled by year and month, yyyymm.nc : \
ex. `197801.nc`

This directory contains to following date range: `1978-01 through 2020-12`

### Check Spatial Coverage of Monthly Files

The monthly averages appear to cover the full domain of th FVCOM mesh.

Note:
There is a note in Matt's code about an issue with lon/lat details not saving for the earlier years:  https://github.com/dzaugis/Ecosystem_Indicators/blob/6d21e553614cb06eb7ea02e4546535cf038d7678/Code/FVCOM_shp_extract.Rmd#L42C42-L42C43 


```{r}
# We can Access Netcdf Files Directly
box_fvcom <- cs_path("res", "FVCOM/FVCOM_gom3_mon_means")
yr <- "2010"
mon <- "01"

# Build the full path
nc_name <- str_c(box_fvcom, yr, mon, ".nc")

# Open (lazy-load) the netcdf connection
x <- nc_open(nc_name)
```


```{r}

# Can also get the mesh itself as a simple feature collection
mesh <- get_mesh_geometry(x, what = 'lonlat') 

# And we can use the mesh to request variables with it
plot(sf::st_geometry(mesh), 
     border = scales::alpha("gray", 0.6), 
     main = "GMRI Monthly FVCOM, Coverage")

```


### Monthly Averaged Variables

It seems like all the variables were brought along.

```{r}
list_vars(x) %>% gt::gt()
```


### Time

We should have one time-step, just confirming that

```{r}
fvcom_time(x) %>% as.Date()
```

### Depth: siglev/siglay

```{r}
# x$dim$siglev
# x$dim$siglay


# Here is how matt handled surface and bottom indexing:
sigLevel <- x$dim$siglay$len
sur_temp <- ncdf4::ncvar_get(x, varid = "temp")[,1]
bot_temp <- ncdf4::ncvar_get(x, varid = "temp")[,sigLevel]

# From here we have surface temperature and bottom temperature as vectors
plot(sur_temp, bot_temp, xlab = "Surface Temp", ylab = "Bottom Temp")

# We need the lat/lon node information as well
lon <- ncdf4::ncvar_get(x, varid = "lon")
lat <- ncdf4::ncvar_get(x, varid = "lat")


# And we need the surface Currents as well
u <- ncdf4::ncvar_get(x, varid = "u")[,1]
v <- ncdf4::ncvar_get(x, varid = "v")[,1]

# And the coordinates for the zonal centers
lonc <- ncdf4::ncvar_get(x, varid = "lonc")
latc <- ncdf4::ncvar_get(x, varid = "latc")

```



---

## Validating fvcom::mesh matches ncvar_get indices

Here is the critical unknown. Are the node ID's that come from the mesh simple feature collections that Ben's package generate able to be passed directly as index numbers to these variable vectors?



### What Does {fvcom} generate as p1/p2/p3?


```{r}
# Do the cropping routine directly on the mesh for these monthly files
 
# Read things in
vts_poly            <- read_sf(str_c(proj_path, "Ecological Data/Spatial_Boundaries/VTSsurvey_nearshore_area.geojson"))
res_shapes         <- cs_path("res", "Shapefiles")
epu_path           <- str_c(res_shapes, "EPU/")
gom_poly           <- read_sf(str_c(epu_path, "individual_epus/GOM.geojson"))
gb_poly            <- read_sf(str_c(epu_path, "individual_epus/GB.geojson"))
shelf_poly         <- read_sf(str_c(epu_path, "EPU_extended.shp"))
st_crs(shelf_poly) <- st_crs(gom_poly)


#--------------------
# Prep CRS for polygons
gom_poly <- st_transform(gom_poly, st_crs(mesh))
gb_poly    <- st_transform(gb_poly, st_crs(mesh))
vts_poly   <- st_transform(vts_poly, st_crs(mesh))



# Flag the locations that are within the domains
mesh_trim <- function(mesh, domain){
  
  # Will remove these columns after st_join
  names_drop <- names(domain)[which(names(domain)!="geometry")] 
  new_mesh <- mesh %>%
    st_join(domain, join = st_within) %>% 
    drop_na() %>% 
    dplyr::select(-all_of(names_drop)) 
  return(new_mesh)
}

# Flag the locations that are within the domain
vts_mesh <- mesh_trim(mesh = mesh, domain = vts_poly)

# Plot how the clipping looks on a map
ggplot() +
  geom_sf(data = vts_mesh, aes(color = "FVCOM Mesh")) +
  geom_sf(data = vts_poly, aes(color = "VTS Survey Area"), fill = "transparent") +
  labs("Mesh Triangles Within Gulf of Maine")


```

The node ID's match up, but we need to be careful and be sure that we're using the right mesh for the right dataset. The FVCOM mesh has evolved with different model iterations so being sure that we have the right mesh will be important.

```{r}
# Do the node ids work this time? YES, so we need to be careful so that we match the mesh to the dataset

# We need the lat/lon node information as well
lon <- ncdf4::ncvar_get(x, varid = "lon")
lat <- ncdf4::ncvar_get(x, varid = "lat")

# Mesh has index information for zonal elements and nodes
node_ids <- st_drop_geometry(vts_mesh) %>% 
  select(-elem) %>% 
  pivot_longer(cols = everything(), 
               names_to = "triangle_point", 
               values_to = "node_id") %>% 
  distinct(node_id) %>% 
  pull()


# Subset lat & lon using those indices
lat_vts  <- lat[node_ids]
lon_vts  <- lon[node_ids]


data.frame(x = lon_vts, y = lat_vts) %>% 
  ggplot() +
  geom_sf(data = vts_mesh, aes(color = "Desired Nodes"))+
  geom_point(aes(x, y, color = "Nodes Indexed")) +
  labs(title = "Success, nodes do match the index id's when mesh is derived from correct dataset")
```


---

# Building VTS Mesh Monthly Inventory

Just get surface and bottom salinity, for the nodes that fall within the VTS survey mesh space.

```{r} 
# Clip worked fine, now can we get the ID's to match:
# Or do we need to?
# We can just use fvcom to get variables for each month over the area
# These are the core variables at the nodes
node_var_list <- c('temp', 'salinity')

# Can we pass multiple time indices to get vars
# ncvar_get(x, "time") # Isn't in time units
time_dim <- fvcom_time(x)


# Then Can we Grab what we want?
surface_vars <- get_mesh(
  x, # Dataset lazyloaded with ncdf4 from THREDDS 
  y = 1, # integer, or indices for siglay or siglev (depth indices)
  vars = c(node_var_list),  # Variables we want
  mesh = vts_mesh, # Mesh to get them for
  time = c(1:length(time_dim)), # All time intervals
  )

bottom_vars <- get_mesh(
  x, # Dataset lazyloaded with ncdf4 from THREDDS 
 y = dim(ncvar_get(x, "siglay"))[2], 
  vars = c(node_var_list),  # Variables we want
  mesh = vts_mesh, # Mesh to get them for
  time = c(1:length(time_dim)), # All time intervals
  )

#  Plot Them
p1 <- ggplot(surface_vars) +
  geom_sf(aes(fill = temp), color = "white") +
  scale_fill_distiller(palette = "RdBu", limits = c(0,9)) +
  map_theme() + 
  labs(title = str_c("Surf Temp at ", time_dim[1]))

#  Plot Them
p2 <- ggplot(bottom_vars) +
  geom_sf(aes(fill = temp), color = "white") +
  scale_fill_distiller(palette = "RdBu", limits = c(0,9)) +
  map_theme() + 
  labs(title = str_c("Bottom Temp at ", time_dim[1]))


p1 / p2


```

## Do we need to regrid to average?

This is the last step to go from the monthly averages in the mesh to a timeseries product

```{r}
# Yea probably
# Here is the regrid and the conversion to a df
# needs to be done for the surface and bottom stacks

# Regrid
var_stack <- raster::stack(
  sapply(
   node_var_list, 
   function(f) { fvcom::rasterize(bottom_vars, field = f) }, 
  simplify = FALSE))

# Get the average of the stack,
# append "surf_" to the names for combining with bottom later
var_df <- var_stack %>% 
  cellStats(mean, na.rm = T) %>% 
  t() %>% 
  as.data.frame() %>% 
  setNames(str_c("surf_", names(.)))

# This will need "var_stack" to be the bottom variable stack ultimately
# just pretending here for convenience
bot_df <- var_stack %>% 
  cellStats(mean, na.rm = T) %>% 
  t() %>% 
  as.data.frame() %>% 
  setNames(str_c("bot_", names(.)))

bind_cols(var_df, bot_df)



```



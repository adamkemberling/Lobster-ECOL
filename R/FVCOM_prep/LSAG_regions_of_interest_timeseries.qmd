---
title: "Lobster Stock Assessment Regions"
description: | 
  Processing Regional Timeseries of Surface and Bottom Temperatures
date: "Updated on: `r Sys.Date()`"
format: 
  html:
    code-fold: true
    code-tools: true
    df-print: kable
    self-contained: true
execute: 
  echo: true
  warning: false
  message: false
  fig.align: "center"
  comment: ""
---

# About: Processing Timeseries of Surface/Bottom Temeprature

For this project we need to use FVCOM for point-value interpolations (what was the temperature at this location, at this time), and for regional summaries (what was the average temperature over time for this area).

This document steps through the approach to getting regional sumaries for our areas of interest. The areas of interest for this project are a combination of nearshore areas (12nm buffers from shore intersections with lobster management strata) and offshore regions (Gulf of Maine, Georges Bank, EPUS). 

```{r}
####. packages. ####
library(gmRi)
library(tidyverse)
library(sf)
library(rnaturalearth)
library(fvcom)
library(ncdf4)

# Set the theme
theme_set(theme_bw() + map_theme())

# Project paths
lob_ecol_path <- cs_path("mills", "Projects/Lobster ECOL")
fvcom_path <- cs_path("res", "FVCOM/Lobster-ECOL")
poly_paths <- here::here("local_data", "OceanModelValidationPolygons")

# Shapefiles
new_england <- ne_states("united states of america", returnclass = "sf") %>% 
  filter(postal %in% c("VT", "ME", "RI", "MA", "CT", "NH", "NY", "MD", "VA", "NJ", "DE", "NC", "PA", "WV"))
canada <- ne_states("canada", returnclass = "sf")

```



```{r}
#| label: style-sheet
#| results: asis

# Use GMRI style
use_gmri_style_rmd()

```




```{r}
#| label: fonts-config
#| echo: false
library(showtext)

# Path to the directory containing the font file (replace with your actual path)
font_dir <- paste0(system.file("stylesheets", package = "gmRi"), "/GMRI_fonts/Avenir/")

# Register the font
font_add(
  family = "Avenir",
  file.path(font_dir, "LTe50342.ttf"),
  bold = file.path(font_dir, "LTe50340.ttf"),
  italic = file.path(font_dir, "LTe50343.ttf"),
  bolditalic = file.path(font_dir, "LTe50347.ttf"))

# Load the font
showtext::showtext_auto()

```



### Load Regional Shapefiles

Regions of interest are looking at temperature dynamics over three regions of interest:

 1. Southern New England - Inshore
 2. Southern New England - Offshore
 3. Gulf of Maine


```{r}
# Name and load the areas:
area_paths <- list.files(poly_paths, full.names = T, pattern = ".shp", recursive = T)
area_names <- str_remove(list.files(poly_paths , pattern = ".shp", recursive = T), ".shp.xml|.shp")
area_names <- area_names[!str_detect(area_paths, ".shp.xml")]
area_paths <- area_paths[!str_detect(area_paths, ".shp.xml")]
lob_areas <- setNames(area_paths, area_names) %>% 
  map(~read_sf(.x) )


# # Map the problem Child
# ggplot() +
#   geom_sf(data = lob_areas$`inshore SNE/OceanProductPoly_SNEinshore`, aes(fill = factor(FID)), alpha = 0.4, show.legend = F) +
#   geom_sf(data = new_england) +
#   geom_sf(data = canada) +
#   theme(legend.position = "right") +
#   scale_fill_gmri() +
#   coord_sf(xlim = c(-75, -69), ylim = c(38, 42)) +
#   theme_bw() + map_theme() +
#   labs(fill = "Area")



# Do a union of those areas
lob_areas_filled <- map(
  lob_areas,
  ~.x %>% 
      st_make_valid %>% 
      st_union() %>%  
      st_transform(st_crs(4269)) %>% 
      st_as_sf())



# Map everything
ggplot() +
  geom_sf(data = bind_rows(lob_areas_filled, .id = "area_id"), aes(fill = area_id), alpha = 0.4) +
  #geom_sf(data = lob_areas$`inshore SNE/OceanProductPoly_SNEinshore`, aes(fill = factor(FID)), alpha = 0.4, show.legend = F) +
  geom_sf(data = new_england) +
  geom_sf(data = canada) +
  theme(legend.position = "right") +
  scale_fill_gmri() +
  coord_sf(xlim = c(-75, -66), ylim = c(38, 45)) +
  theme_bw() + map_theme() +
  labs(fill = "Area")
```

### Open Daily FVCOM Data

Daily surface and bottom temperatures were obtained through direct communication with the Dr. Chen's group at UMASS Dartmouth, with special thanks for Drs. Wang & Li for their help and continued correspondence.

From these daily-averagede NetCDF files and using the FVCOM r-package it should be possible to load the GOM3 triangular mesh as a simple features dataframe into R.

```{r}

# Here are the files we have, loop over them later
fvcom_surfbot_files <- setNames(
  list.files(fvcom_path, full.names = T, pattern = ".nc"),
  str_remove(list.files(fvcom_path, full.names = F, pattern = ".nc"), ".nc"))


# Test File: GOM3 1978
# Load some daily FVCOM that we downloaded and averaged
fvcom_yrx <- nc_open(fvcom_surfbot_files["gom3_1978"])



# Get the mesh itself as a simple feature collection
gom3_mesh <- get_mesh_geometry(fvcom_yrx, what = 'lonlat')

# Get the mesh within
gom3_mesh_inside <- gom3_mesh %>% 
  st_transform(st_crs(4269)) %>% 
  st_intersection(., st_union(bind_rows(lob_areas_filled)))



# Map everything
ggplot() +
  geom_sf(data = gom3_mesh_inside, alpha = 0.4, linewidth = 0.1) +
  geom_sf(data = bind_rows(lob_areas_filled, .id = "area_id"), aes(fill = area_id), alpha = 0.4) +
  geom_sf(data = new_england) +
  geom_sf(data = canada) +
  theme(legend.position = "right") +
  scale_fill_gmri() +
  coord_sf(xlim = c(-75, -65), ylim = c(38, 45)) +
  theme_bw() + map_theme() +
  labs(
    title = "Coverage overlap of FVCOM and study regions",
    fill = "Area")

```

### Overlay Regions

For each region we will need to perform an intersection with the FVCOM mesh. From that geoprocessing operation we can then determine the node & element ID's to pull out of the netcdf files.

At this step we can also calculate relative areas of resulting triangles and fractions of triangles to use for weighted-averages for regional summaries.

```{r}
#| label: node-element-assignments

# Nodes can be acquired this way
node_ids <- fvcom_nodes(fvcom_yrx) %>% 
  rename(node_id = node) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326)

# Center element ids can be gained this way
elem_ids <- fvcom_elems(fvcom_yrx) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326)


# We want both the ids to join, but we also want to crop it...
lob_areas_trans <- map(
  lob_areas_filled, 
  ~st_transform(.x, crs = 6318) %>% st_make_valid())



# Map what is going on in the next steps
# everything below steps through the steps
crop_t <- st_intersection(
  x = gom3_mesh, 
  y = lob_areas_trans$`inshore SNE/OceanProductPoly_SNEinshore`) 


# Map what is happening:

ggplot() +
  geom_sf(data = new_england) +
  geom_sf(data = crop_t, aes(color = "FVCOM Mesh"), 
          alpha = 0.5) +
  geom_sf(data = lob_areas_trans$`inshore SNE/OceanProductPoly_SNEinshore`, 
          aes(color = "511 - Eastern Maine"), linewidth = 0.8,
          alpha = 0.5, fill = "transparent") +
  # highlight one node:
  geom_sf(data = crop_t[20,], color = "black", fill = "orange") +
  geom_segment(
    data = data.frame(
      x = -74, xend = -73, y = 39, yend = 39),
    aes(x, y, xend = xend, yend = yend)) +
  geom_text(
    data = data.frame(
      x = -72.25, y = 39,
      label = str_c("FVCOM Element: ", st_drop_geometry(crop_t[20,"elem"]))),
    aes(x, y, label = label)) +
  scale_color_gmri() +
  map_theme() +
  # coord_sf(xlim = c(-74.4, -70), ylim = c(38.8, 41.75)) +
  coord_sf(xlim = c(-71.5, -70.6), ylim = c(41.8, 41)) +
  labs(
    title = "Assigning node/elements and area-weights to regions",
    subtitle = "st_intersection(mesh, area_poly)",
       color = "Geometry Source")


#
```


# Perform Intersections on All Polygons


```{r}
#| eval: false
#| label: step 1 polygon overlay

# Run them all and save tables in- case we need to operate in python
lob_areas_intersections <- map_dfr(lob_areas_trans, function(x){
  
  # Run intersection
  mesh_clip <- st_intersection(gom3_mesh, x) 
  
  # Get areas
  # Pull necessary information out
  mesh_clip_df <- mesh_clip %>% 
    mutate(rel_area = st_area(mesh_clip)) %>% 
    #select(SHORT_NAME, Id_1, FULL_NAME, elem, p1, p2, p3, rel_area) %>% 
    st_drop_geometry()
  
  # Return
  return(mesh_clip_df)
  
}, .id = "lob_area")



# # Save these out for lookup tables
# write_csv(lob_areas_intersections, here::here("local_data/jkipp_areas_mesh_weights.csv"))


```



```{r}
#| label: load polygon assignments


# Load what we did above
lob_areas_zones <- read_csv(here::here("local_data/jkipp_areas_mesh_weights.csv"))


# Put the inshore and offshore together
regional_assignments <- lob_areas_zones %>% 
  dplyr::select(area_id = lob_area, elem, p1, p2, p3, rel_area) %>% 
  mutate(rel_area = as.numeric(rel_area))


# From intersection we need to add areas
regional_assignments %>% 
  st_drop_geometry() %>% 
  head() %>% 
  gt::gt() %>% gt::tab_header(
    title = "Intersection Insight gained:",
    subtitle = "Information needed to subset from NetCDF and weight appropriately for regional statistics")

```

# Get triangle-specific averages of nodal values

For surface and bottom temperatures the values are stored/calculated at the triangle vertices, or nodes. For regional statistics like the regional average temperature, we need to average the three nodes to get a value representative of the interior space.

This step loops through all the regional elements that are within any of our regions and processes that value once for each. These are then stored in a named list that can be accessed later.

```{r}
#| label: regional_from_nodes function


# Now we want to pull all values on the time dimension 
# for the key nodes we need, the ones that surround all unique
# elements after the st_intersecction()



#' @Title FVCOM Element Average from Vertices
#' 
#' 
#' @description Get the average value from the relevant vertices, along  
#' the time dimension for FVCOM mesh triangles.
#'
#' @param fvcom_mesh_trios
#' @param nc
#' @param nc_varname 
#'
#' @return
#' @export
#'
#' @examples
regional_from_nodes <- function(fvcom_mesh_trios, nc, nc_varname = "surface_t"){
  
  
  # Take the unique elements from our table:
  unique_elems <- distinct(fvcom_mesh_trios, elem) %>% pull(elem)
  
  # Make a named list
  unique_elems <- setNames(unique_elems, unique_elems)
  
  # Iterate through each unique element (triangle element, p1, p2, p3)
  # Average the nodes to get value for each element
  triangle_temps <- map(unique_elems, function(elem_id){
    
    # Slice the row for that element, get distinct in case >1
    elem_df <- filter(regional_assignments, elem == elem_id) %>% 
      distinct(elem, p1, p2, p3)
    
    # Get element id
    elem_num <- elem_df[[1, "elem"]]
    
    # Pull surface_t for the three nodes
    p1_ts <- ncvar_get(
      nc, 
      varid = nc_varname, 
      start = c(elem_df[[1, "p1"]], 1), 
      count = c(1, -1))
    p2_ts <- ncvar_get(
      nc, 
      varid = nc_varname, 
      start = c(elem_df[[1, "p2"]], 1), 
      count = c(1, -1))
    p3_ts <- ncvar_get(
      nc, 
      varid = nc_varname, 
      start = c(elem_df[[1, "p3"]], 1), 
      count = c(1, -1))
    
    # Get Averages
    elem_avg_var <- (p1_ts + p2_ts + p3_ts) / 3
    
    # Spit it out, no dataframe bs
    unique_elems[elem_num] <- elem_avg_var
    
  })
  
}
```

```{r}
#| label: test regional_from_nodes
#| eval: false

# Run that for all the years I guess?!?
triangle_avg_1978 <- regional_from_nodes(
  fvcom_mesh_trios = regional_assignments, 
  nc = fvcom_yrx, 
  nc_varname = "surface_t")
# Sys.time()
```

# Area-weighted Regional Averages

Within each polygon overlay, we want to weight the averages of each triangle we just estimated by their relative areas. This will give us the average value for the entire polygon, appropriately weighting each of the component areas.

```{r}
#| label: fvcom_regional_averages function


# Take the code above and calculate the weighted averages:
# function should take:
# 1. region overlay dataframe, containing relative areas for weights
# 2. triangle-specific values





#' @title Regional Averages from FVCOM-Polygon Intersection
#' 
#' @description
#'
#' @param mesh_poly_intersection 
#' @param regional_means 
#' @param nc 
#'
#' @return
#' @export
#'
#' @examples
fvcom_regional_averages <- function(mesh_poly_intersection, regional_means, nc){
  
  # Get the elements
  poly_elems <- as.character(mesh_poly_intersection$elem)

  # pull out the weights, name them
  poly_wts <- as.numeric(mesh_poly_intersection$rel_area) %>% 
    setNames(poly_elems)
  
  # Weight each timeseries by area, then sum them all
  # Get the timeseries that go with the triangles
  # multiply the relevant relative areas
  poly_wtd_ts <- purrr::map(poly_elems, function(x){
    wtd_vals <- regional_means[[x]] * poly_wts[[x]]
    return(wtd_vals)}) %>% 
    reduce(.x = ., .f = `+`)
  
  # Divide by total area
  poly_tot_area <- sum(as.numeric(mesh_poly_intersection$rel_area))
  
  # Add the time dimension and return as dataframe
  poly_regional <- data.frame(
    "time"     = as.Date(ncvar_get(nc, "Times")),
    "regional_mu" = poly_wtd_ts / poly_tot_area)
  return(poly_regional)
  
}
```


# Run for All Years, Save.

Before this step, need to perform the `st_intersection` step that gathers the information on which mesh triangles to get averages from, and how to weight them appropriately.

Then we're just looping through opening the files for each year and the dataframes for each area.

Everything below could be done in one step instead of a loop with xarray

### Surface Temperature Processing

```{r}
#| label: process surface temperatures
#| eval: false


# Process Surface Temperature for all the regions:
fvcom_var_name <- "surface_t"


# Time it
Sys.time()
all_region_surface_temp <- map_dfr(fvcom_surfbot_files, function(nc_x){
  
  # 1. Open the netcdf:
  yr_x_fvcom <- nc_open(nc_x)
  
  # 2. Get the average values for the triangles
  # Run that for all the years I guess?!?
  triangle_avgs <- regional_from_nodes(
    fvcom_mesh_trios = regional_assignments, 
    nc = yr_x_fvcom, 
    nc_varname = fvcom_var_name)
  
  
  # Get the regional averages for each region/group
  poly_avgs_yr_x <-  regional_assignments %>% 
    split(.$area_id) %>% 
    map_dfr(
    .x = .,
    .f = ~fvcom_regional_averages(
      mesh_poly_intersection = .x,
      regional_means = triangle_avgs, 
      nc = yr_x_fvcom),
    .id = "area_id"
  )
  
  # Close the netcdf connection
  nc_close(yr_x_fvcom)
  
  # Return the regional averages
  return(poly_avgs_yr_x)
  
})

# time completion
Sys.time()


# # Save them individually
# fvcom_regional_surface_temps %>% 
# # all_region_surface_temp %>%
#   # rename(surface_temp = regional_mu) %>%
#   split(.$area_id) %>%
#   iwalk(
#     function(.x,.y){
#       write_csv(.x, str_c(lob_ecol_path, "FVCOM_processed/area_timeseries/surface_temperature/", .y, ".csv"))
#     }
#     )


# # Plot
# all_region_surface_temp %>% 
#   mutate(time = as.Date(time)) %>% 
#   ggplot() +
#   geom_line(aes(time, regional_mu, color = area_id),
#             linewidth = 0.4, alpha = 0.4)


```



### Bottom Temperature Processing

```{r}
#| label: process bottom temperatures
#| eval: false


# Process Surface Temperature for all the regions:
fvcom_var_name <- "bottom_t"


# Time it
Sys.time()
all_region_bottom_temp <- map_dfr(fvcom_surfbot_files, function(nc_x){
  
  # 1. Open the netcdf:
  yr_x_fvcom <- nc_open(nc_x)
  
  # 2. Get the average values for the triangles
  # Run that for each of the years once
  triangle_avgs <- regional_from_nodes(
    fvcom_mesh_trios = regional_assignments, 
    nc = yr_x_fvcom, 
    nc_varname = fvcom_var_name)
  
  
  # Get the regional averages for each region/group
  poly_avgs_yr_x <-  regional_assignments %>% 
    split(.$area_id) %>% 
    map_dfr(
    .x = .,
    .f = ~fvcom_regional_averages(
      mesh_poly_intersection = .x,
      regional_means = triangle_avgs, 
      nc = yr_x_fvcom),
    .id = "area_id"
  )
  
  # Close the netcdf connection
  nc_close(yr_x_fvcom)
  
  # Return the regional averages
  return(poly_avgs_yr_x)
  
})

# time completion
Sys.time()



# Plot
all_region_bottom_temp %>% 
  mutate(time = as.Date(time)) %>% 
  ggplot() +
  geom_line(aes(time, regional_mu, color = area_id),
            linewidth = 0.4, alpha = 0.4)
```

### Combine

```{r}

# Reshape/format them to put together
# Put them into one table:
regional_temperatures <- left_join(
  all_region_surface_temp  %>% 
    mutate(time = as.Date(time)) %>% 
    rename(surface_t = regional_mu), 
  all_region_bottom_temp  %>% 
    mutate(time = as.Date(time)) %>% 
    rename(bottom_t = regional_mu)) %>% 
  mutate(area_id = str_split_i(area_id, "/", 2))


# Plot to look
regional_temperatures %>% 
  ggplot() +
  geom_line(aes(time, surface_t, color = "Surface Temperature"), linewidth = 0.8, alpha = 0.6) +
  geom_line(aes(time, bottom_t, color = "Bottom Temperature"), linewidth = 0.8, alpha = 0.6) +
  scale_color_gmri() +
  facet_wrap(~area_id, nrow = 2)


# # # Exporting
# write_csv(
#   regional_temperatures,
#   here::here("local_data", "jkipp_areas_fvcom_temperatures_daily.csv"))


# # Load the collection if necessary:
# regional_temperatures <- read_csv(here::here("local_data", "jkipp_areas_fvcom_temperatures_daily.csv"))


```


## How many Nodes/Elements

As another layer of information, it may be useful to know how many nodes/elements (proxies for resolution) contributed to mean estimates:

```{r}

# This gets the average area of the polygons within the strata
# Be careful with the units here! relative area is in meters, units come along for the ride
area_resolutions <- lob_areas_intersections %>% 
  group_by(area_id = lob_area) %>% 
  summarise(n_elems = n_distinct(elem),
            avg_area_km2 =  as.numeric(mean(rel_area)/1e6))

# We also probably want the number of nodes, and center elements
# we can get the center coordinates this way
gom3_mesh_elems <- data.frame(
  "lonc" = ncvar_get(fvcom_yrx, "lonc"),
  "latc" = ncvar_get(fvcom_yrx, "latc")) %>% 
  distinct(lonc, latc) %>% 
  mutate(elem = row_number()) %>% 
  st_as_sf(coords = c("lonc", "latc"), 
           crs = st_crs(gom3_mesh), 
           remove = F)
# and the nodes this way
gom3_mesh_nodes <- data.frame(
  "lon" = ncvar_get(fvcom_yrx, "lon"),
  "lat" = ncvar_get(fvcom_yrx, "lat")) %>% 
  mutate(node = row_number()) %>% 
  st_as_sf(coords = c("lon", "lat"), 
           crs = st_crs(gom3_mesh), 
           remove = F)

# validate we have the node/center coordinates right
ggplot() +
  geom_sf(data = gom3_mesh_nodes, shape = 3, size = 0.4, alpha = 0.6, color = "#ebcb27" ) +
  geom_sf(data = gom3_mesh_elems, shape = 3, size = 0.4, alpha = 0.6, color = "#07a3b7") + 
  theme_dark() +
  theme(panel.grid = element_blank()) +
  # coord_sf(xlim = c(-71.5, -69.5), ylim = c(41, 42.5)) # Cape Cod
  #coord_sf(xlim = c(-70.5, -68), ylim = c(43.5, 44.25)) # Maine
  coord_sf(xlim = c(-70.35, -67.25), ylim = c(43.25, 44.65)) # Maine


# Use crop to count the points within each one
# Or use intersection?
area_points <- map_dfr(
  .x = lob_areas_trans, 
  .f = possibly(function(strata_x){
  n_nodes <- st_intersection(x = gom3_mesh_nodes, y = strata_x) %>% 
  #n_nodes <- st_crop(x = gom3_mesh_nodes, y = strata_x) %>% 
      nrow()
  n_centers <- st_intersection(x = gom3_mesh_elems, y = strata_x) %>% 
  #n_centers <- st_crop(x = gom3_mesh_elems, y = strata_x) %>% 
      nrow()
  tibble(
    "n_nodes" = n_nodes,
    "n_centers" = n_centers)
  }, 
  otherwise = tibble("n_nodes" = 0, "n_centers" = 0)), 
  .id = "area_id")



# totally missed it, use the new trick i learned
#strata_points <- .Last.value




# Join them together:
area_resolution_data <- left_join(
  mutate(area_resolutions, area_id = as.character(area_id)),
  y = area_points) %>% 
  mutate(area_id = str_split_i(area_id, "/", 2))



# Check the distribution
#hist(strata_resolution_data$avg_area_km2)

# Mqp the avg resolution in each strata
bind_rows(lob_areas_trans, .id = "area_id") %>% 
  mutate(area_id = str_split_i(area_id, "/", 2)) %>% 
  left_join(area_resolution_data) %>% 
  mutate(avg_area_km2 = as.numeric(avg_area_km2)) %>% 
  ggplot() +
  geom_sf(
    data = gom3_mesh_inside, 
    color = "gray60", alpha = 0.2, linewidth = 0.1) +
  geom_sf(aes(fill = avg_area_km2), alpha = 0.3) +
  scale_fill_distiller(palette = "RdYlGn")



# # Save that out:
# write_csv(area_resolution_data, here::here("local_data", "jkipp_areas_fvcom_elem_resolution.csv"))

# # check it
# read_csv(here::here("local_data", "jkipp_areas_fvcom_elem_resolution.csv"))
```



# Inspecting Timeseries

Prior to any interpolations, timeseries for both inshore and offshore areas are evaluated for both long-term trends (Kendall test), and structural breaks in means/trends (changepoint/breakpoint tests).

These tests determine the following:\
- Significant trends = non-stationary, and global krigging model should not be used\
- Breakpoints determine which groups of years should be used for seasonal kriging models

```{r}
#| label: both-version-inshoreoffshore

# Load and add inshore/offshore labels on the daily data
regional_temperatures <- read_csv(
  str_c(lob_ecol_path, "FVCOM_processed/area_timeseries/jkipp_regions_fvcom_temperatures_daily.csv"))

```

---

# Prepare Gulf of Maine Area for Jeff

Jeff would like the daily data for August-December for the Gulf of Maine, but at the daily resolution.

It may make sense to send it to him as triangle averages, using their ID's, and a separate .geojson or .shp file that has the triangles themselves.



### GOM Bottom Temperature Processing


```{r}
# We can see all the triangles that are at play this way
regional_assignments %>% filter(area_id == "GOM/OceanProductPoly_GOM") %>% distinct(elem)


```


```{r}
#| label: process bottom temperatures
#| eval: false


# Process Surface Temperature for all the regions:
fvcom_var_name <- "bottom_t"



# Time it
Sys.time()
all_element_bottom_temp <- fvcom_surfbot_files %>% 
  map_dfr(
    .x = ., 
    function(nc_x){
  
  # 1. Open the netcdf:
  yr_x_fvcom <- nc_open(nc_x)
  
  # 2. Get the average values for the triangles
  # Run that for each of the years once
  triangle_avgs <- regional_from_nodes(
    fvcom_mesh_trios = regional_assignments %>% 
      filter(area_id == "GOM/OceanProductPoly_GOM"), 
    nc = yr_x_fvcom, 
    nc_varname = fvcom_var_name)
  
  # Get the time dimension
  fv_dates <- data.frame(
    "time" = ncvar_get(yr_x_fvcom, "Times"))
  
  # Bind and filter the dates
  triangle_avgs <- bind_cols(fv_dates, triangle_avgs) %>% 
    filter(month(time) >= 8)
  
  # Close the netcdf connection
  nc_close(yr_x_fvcom)
  
  # Return the regional averages
  return(triangle_avgs)
  
})

# time completion
Sys.time()
```


```{r}
# Plot a day
day_test <- "2010-12-25"
day_test <- "2010-08-25"
all_element_bottom_temp %>% #dim()
  mutate(time = as.Date(time)) %>% 
  filter(time == day_test) %>% 
  pivot_longer(cols = -time, names_to = "elem", values_to = "bottom_t") %>% 
  mutate(bottom_t = as.numeric(bottom_t)) %>% 
  left_join(mutate(gom3_mesh, elem = as.character(elem))) %>% 
  st_as_sf() %>% 
  ggplot() +
  #geom_sf(data = new_england, color = "gray30") +
  geom_sf(aes(fill = bottom_t), color = "transparent") +
  scale_fill_distiller(
    palette = "RdBu") +
  theme_dark() +
  theme(legend.position = "bottom", legend.title.position = "top", legend.title = element_text(hjust = 0.5)) +
  coord_sf(expand = F) +
  labs(title = str_c("FVCOM Daily Bottom Temperature:"),
       fill = "Bottom Temperature",
       subtitle = str_c("Date: ", day_test))
```


```{r}
#| eval: false
# Save the pieces

# This is the bottom temperature for those periods for that area
jkipp_gom_bt <- all_element_bottom_temp %>% #dim()
  mutate(time = as.Date(time)) %>% 
  pivot_longer(
    cols = -time, 
    names_to = "elem", 
    values_to = "bottom_t") %>% 
  mutate(elem = as.character(elem),
         bottom_t = as.numeric(bottom_t))

# # This is the daily bottom temperature that Jeff wanted
# write_csv(
#   jkipp_gom_bt,
#   here::here("local_data/dailyfvcomBT_OceanProductPoly_GOM_AugDec.csv")
# )
# 
# 
# # this is the shapefile information
# sf::write_sf(gom3_mesh, here::here("local_data/FVCOM_gom3_mesh.geojson"))

# mesh_test <- st_read(here::here("local_data/FVCOM_gom3_mesh.geojson"))
# ggplot(mesh_test) +geom_sf()
```


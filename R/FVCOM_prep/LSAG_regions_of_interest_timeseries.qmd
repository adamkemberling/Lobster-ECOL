---
title: "Lobster Stock Assessment Regions"
description: | 
  Processing Regional Timeseries of Surface and Bottom Temperatures
date: "Updated on: `r Sys.Date()`"
format: 
  html:
    code-fold: true
    code-tools: true
    df-print: kable
    self-contained: true
execute: 
  echo: true
  warning: false
  message: false
  fig.align: "center"
  comment: ""
---

# About: Processing Timeseries of Surface/Bottom Temeprature

For this project we need to use FVCOM for point-value interpolations (what was the temperature at this location, at this time), and for regional summaries (what was the average temperature over time for this area).

This document steps through the approach to getting regional sumaries for our areas of interest. The areas of interest for this project are a combination of nearshore areas (12nm buffers from shore intersections with lobster management strata) and offshore regions (Gulf of Maine, Georges Bank, EPUS). 

```{r}
####. packages. ####
library(gmRi)
library(tidyverse)
library(sf)
library(rnaturalearth)
library(fvcom)
library(ncdf4)
library(patchwork)

# Set the theme
theme_set(theme_bw() + map_theme())

# Project paths
lob_ecol_path <- cs_path("mills", "Projects/Lobster ECOL")
fvcom_path    <- cs_path("res", "FVCOM/Lobster-ECOL")
poly_paths    <- here::here("local_data", "OceanModelValidationPolygons")

# Shapefiles
new_england <- ne_states("united states of america", returnclass = "sf") %>% 
  filter(postal %in% c("VT", "ME", "RI", "MA", "CT", "NH", "NY", "MD", "VA", "NJ", "DE", "NC", "PA", "WV"))
canada <- ne_states("canada", returnclass = "sf")

```



```{r}
#| label: style-sheet
#| results: asis

# Use GMRI style
use_gmri_style_rmd()

```




```{r}
#| label: fonts-config
#| echo: false
library(showtext)

# Path to the directory containing the font file (replace with your actual path)
font_dir <- paste0(system.file("stylesheets", package = "gmRi"), "/GMRI_fonts/Avenir/")

# Register the font
font_add(
  family = "Avenir",
  file.path(font_dir, "LTe50342.ttf"),
  bold = file.path(font_dir, "LTe50340.ttf"),
  italic = file.path(font_dir, "LTe50343.ttf"),
  bolditalic = file.path(font_dir, "LTe50347.ttf"))

# Load the font
showtext::showtext_auto()

```



### Load Regional Shapefiles

Regions of interest are looking at temperature dynamics over three regions of interest:

 1. Southern New England - Inshore
 2. Southern New England - Offshore
 3. Gulf of Maine


```{r}

# Get Paths and Poly Names to process:
area_paths <- list.files(poly_paths, full.names = T, pattern = ".shp", recursive = T)
area_paths <- area_paths[!str_detect(area_paths, ".shp.xml")]


area_names <- list.files(poly_paths , pattern = ".shp", recursive = T)
area_names <- area_names[!str_detect(area_names, ".shp.xml")] %>%  str_remove(".shp")  %>%  
  str_split_i(pattern = "/", i = 2)



# Load them in a list
lob_areas <- setNames(area_paths, area_names) %>% 
  map(~read_sf(.x) )


# Do a union of those areas, SNE is multipolygon
lob_areas_filled <- map(
  lob_areas,
  ~.x %>% 
      st_make_valid %>% 
      st_union() %>%  
      st_transform(st_crs(4269)) %>% 
      st_as_sf())



# Map everything
ggplot() +
  geom_sf(data = bind_rows(lob_areas_filled, .id = "area_id"), aes(fill = area_id), alpha = 0.4) +
  #geom_sf(data = lob_areas$`OceanProductPoly_SNEinshore`, aes(fill = factor(FID)), alpha = 0.4, show.legend = F) +
  geom_sf(data = new_england) +
  geom_sf(data = canada) +
  theme(legend.position = "right") +
  scale_fill_gmri() +
  coord_sf(xlim = c(-75, -66), ylim = c(38, 45)) +
  theme_bw() + map_theme() +
  labs(fill = "Area")
```

### Open Daily FVCOM Data

Daily surface and bottom temperatures were obtained through direct communication with the Dr. Chen's group at UMASS Dartmouth, with special thanks for Drs. Wang & Li for their help and continued correspondence.

From these daily-averagede NetCDF files and using the FVCOM r-package it should be possible to load the GOM3 triangular mesh as a simple features dataframe into R.

```{r}

# Here are the files we have, loop over them later
fvcom_surfbot_files <- setNames(
  list.files(fvcom_path, full.names = T, pattern = ".nc"),
  str_remove(list.files(fvcom_path, full.names = F, pattern = ".nc"), ".nc"))


# Test File: GOM3 1978
# Load some daily FVCOM that we downloaded and averaged
fvcom_yrx <- nc_open(fvcom_surfbot_files["gom3_1978"])



# Get the mesh itself as a simple feature collection
gom3_mesh <- get_mesh_geometry(fvcom_yrx, what = 'lonlat')

# Get the mesh within
gom3_mesh_inside <- gom3_mesh %>% 
  st_transform(st_crs(4269)) %>% 
  st_intersection(., st_union(bind_rows(lob_areas_filled)))



# Map everything
ggplot() +
  geom_sf(data = gom3_mesh_inside, alpha = 0.4, linewidth = 0.1) +
  geom_sf(data = bind_rows(lob_areas_filled, .id = "area_id"), aes(fill = area_id), alpha = 0.4) +
  geom_sf(data = new_england) +
  geom_sf(data = canada) +
  theme(legend.position = "right") +
  scale_fill_gmri() +
  coord_sf(xlim = c(-75, -65), ylim = c(38, 45)) +
  theme_bw() + map_theme() +
  labs(
    title = "Coverage overlap of FVCOM and study regions",
    fill = "Area")

```

### Overlay Regions

For each region we will need to perform an intersection with the FVCOM mesh. From that geoprocessing operation we can then determine the node & element ID's to pull out of the netcdf files.

At this step we can also calculate relative areas of resulting triangles and fractions of triangles to use for weighted-averages for regional summaries.

```{r}
#| label: node-element-assignments

# Nodes can be acquired this way
node_ids <- fvcom_nodes(fvcom_yrx) %>% 
  rename(node_id = node) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326)

# Center element ids can be gained this way
elem_ids <- fvcom_elems(fvcom_yrx) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326)


# We want both the ids to join, but we also want to crop it...
lob_areas_trans <- map(
  lob_areas_filled, 
  ~st_transform(.x, crs = 6318) %>% st_make_valid())



# Map what is going on in the next steps
# everything below steps through the steps
crop_t <- st_intersection(
  x = gom3_mesh, 
  y = lob_areas_trans$`OceanProductPoly_SNEinshore`) 


# Map what is happening:

ggplot() +
  geom_sf(data = new_england) +
  geom_sf(data = crop_t, aes(color = "FVCOM Mesh"), 
          alpha = 0.5) +
  geom_sf(data = lob_areas_trans$`OceanProductPoly_SNEinshore`, 
          aes(color = "511 - Eastern Maine"), linewidth = 0.8,
          alpha = 0.5, fill = "transparent") +
  # highlight one node:
  geom_sf(data = crop_t[20,], color = "black", fill = "orange") +
  geom_segment(
    data = data.frame(
      x = -74, xend = -73, y = 39, yend = 39),
    aes(x, y, xend = xend, yend = yend)) +
  geom_text(
    data = data.frame(
      x = -72.25, y = 39,
      label = str_c("FVCOM Element: ", st_drop_geometry(crop_t[20,"elem"]))),
    aes(x, y, label = label)) +
  scale_color_gmri() +
  map_theme() +
  # coord_sf(xlim = c(-74.4, -70), ylim = c(38.8, 41.75)) +
  coord_sf(xlim = c(-71.5, -70.6), ylim = c(41.8, 41)) +
  labs(
    title = "Assigning node/elements and area-weights to regions",
    subtitle = "st_intersection(mesh, area_poly)",
       color = "Geometry Source")


```


# Perform Intersections on All Polygons


```{r}
#| eval: false
#| label: step 1 polygon overlay

# Run them all and save tables in- case we need to operate in python
lsag_area_intersections <- map_dfr(lob_areas_trans, function(x){
  
  # Run intersection
  mesh_clip <- st_intersection(gom3_mesh, x) 
  
  # Get areas
  # Pull necessary information out
  mesh_clip_df <- mesh_clip %>% 
    mutate(rel_area = st_area(mesh_clip)) %>% 
    st_drop_geometry()
  
  # Return
  return(mesh_clip_df)
  
}, .id = "area_id")



# # Save these out for lookup tables
# write_csv(lsag_area_intersections, here::here("local_data/jkipp_regions_mesh_weights.csv"))


```



```{r}
#| label: load polygon assignments


# Load what we did above
lsag_area_intersections <- read_csv(here::here("local_data/jkipp_regions_mesh_weights.csv"))


# Put the inshore and offshore together
lsag_area_intersections <- lsag_area_intersections %>% 
  dplyr::select(area_id, elem, p1, p2, p3, rel_area) %>% 
  mutate(rel_area = as.numeric(rel_area))


# From intersection we need to add areas
lsag_area_intersections %>% 
  st_drop_geometry() %>% 
  head() %>% 
  gt::gt() %>% gt::tab_header(
    title = "Intersection Insight gained:",
    subtitle = "Information needed to subset from NetCDF and weight appropriately for regional statistics")

```

# Get triangle-specific averages of nodal values

For surface and bottom temperatures the values are stored/calculated at the triangle vertices, or nodes. For regional statistics like the regional average temperature, we need to average the three nodes to get a value representative of the interior space.

This step loops through all the regional elements that are within any of our regions and processes that value once for each. These are then stored in a named list that can be accessed later.

```{r}
#| label: regional_from_nodes function


# Now we want to pull all values on the time dimension 
# for the key nodes we need, the ones that surround all unique
# elements after the st_intersecction()



#' @Title FVCOM Element Average from Vertices
#' 
#' 
#' @description Get the average value from the relevant vertices, along  
#' the time dimension for FVCOM mesh triangles.
#'
#' @param fvcom_mesh_trios
#' @param nc
#' @param nc_varname 
#'
#' @return
#' @export
#'
#' @examples
regional_from_nodes <- function(fvcom_mesh_trios, nc, nc_varname = "surface_t"){
  
  
  # Take the unique elements from our table:
  unique_elems <- distinct(fvcom_mesh_trios, elem) %>% pull(elem)
  
  # Make a named list
  unique_elems <- setNames(unique_elems, unique_elems)
  
  # Iterate through each unique element (triangle element, p1, p2, p3)
  # Average the nodes to get value for each element
  triangle_temps <- map(unique_elems, function(elem_id){
    
    # Slice the row for that element, get distinct in case >1
    elem_df <- filter(lsag_area_intersections, elem == elem_id) %>% 
      distinct(elem, p1, p2, p3)
    
    # Get element id
    elem_num <- elem_df[[1, "elem"]]
    
    # Pull surface_t for the three nodes
    p1_ts <- ncvar_get(
      nc, 
      varid = nc_varname, 
      start = c(elem_df[[1, "p1"]], 1), 
      count = c(1, -1))
    p2_ts <- ncvar_get(
      nc, 
      varid = nc_varname, 
      start = c(elem_df[[1, "p2"]], 1), 
      count = c(1, -1))
    p3_ts <- ncvar_get(
      nc, 
      varid = nc_varname, 
      start = c(elem_df[[1, "p3"]], 1), 
      count = c(1, -1))
    
    # Get Averages
    elem_avg_var <- (p1_ts + p2_ts + p3_ts) / 3
    
    # Spit it out, no dataframe bs
    unique_elems[elem_num] <- elem_avg_var
    
  })
  
}
```


# Get Mean of Each Triangle - Get Weighted Average from All

Within each polygon overlay, we want to weight the averages of each triangle we just estimated by their relative areas. This will give us the average value for the entire polygon, appropriately weighting each of the component areas.

```{r}
#| label: fvcom_regional_averages function


# Take the code above and calculate the weighted averages:
# function should take:
# 1. region overlay dataframe, containing relative areas for weights
# 2. triangle-specific values





#' @title Regional Averages from FVCOM-Polygon Intersection
#' 
#' @description
#'
#' @param mesh_poly_intersection 
#' @param mesh_elem_avgs 
#' @param nc 
#'
#' @return
#' @export
#'
#' @examples
fvcom_regional_averages <- function(mesh_poly_intersection, mesh_elem_avgs, nc){
  
  # Get the elements
  poly_elems <- as.character(mesh_poly_intersection$elem)

  # pull out the weights, name them
  poly_wts <- as.numeric(mesh_poly_intersection$rel_area) %>% 
    setNames(poly_elems)
  
  # Weight each timeseries by area, then sum them all
  # Get the timeseries that go with the triangles
  # multiply the relevant relative areas
  poly_wtd_ts <- purrr::map(poly_elems, function(x){
    wtd_vals <- mesh_elem_avgs[[x]] * poly_wts[[x]]
    return(wtd_vals)}) %>% 
    reduce(.x = ., .f = `+`)
  
  # Divide by total area
  poly_tot_area <- sum(as.numeric(mesh_poly_intersection$rel_area))
  
  # Add the time dimension and return as dataframe
  poly_regional <- data.frame(
    "time"     = as.Date(ncvar_get(nc, "Times")),
    "regional_mu" = poly_wtd_ts / poly_tot_area)
  return(poly_regional)
  
}
```


# Full Process: Process Surface+Bottom Timeseries

Before this step, need to perform the `st_intersection` step that gathers the information on which mesh triangles to get averages from, and how to weight them appropriately.

Then we're just looping through opening the files for each year and the dataframes for each area.

Everything below could be done in one step instead of a loop with xarray

### Surface Temperature Processing

```{r}
#| label: process surface temperatures
#| eval: false


# Process Surface Temperature for all the regions:
fvcom_var_name <- "surface_t"


# Time it
Sys.time()
all_region_surface_temp <- map_dfr(fvcom_surfbot_files, function(nc_x){
  
  # 1. Open the netcdf:
  yr_x_fvcom <- nc_open(nc_x)
  
  # 2. Get the average values for the triangles
  # Run that for all the years I guess?!?
  triangle_avgs <- regional_from_nodes(
    fvcom_mesh_trios = lsag_area_intersections, 
    nc = yr_x_fvcom, 
    nc_varname = fvcom_var_name)
  
  
  # Get the regional averages for each region/group
  poly_avgs_yr_x <-  lsag_area_intersections %>% 
    split(.$area_id) %>% 
    map_dfr(
    .x = .,
    .f = ~fvcom_regional_averages(
      mesh_poly_intersection = .x,
      mesh_elem_avgs = triangle_avgs, 
      nc = yr_x_fvcom),
    .id = "area_id"
  )
  
  # Close the netcdf connection
  nc_close(yr_x_fvcom)
  
  # Return the regional averages
  return(poly_avgs_yr_x)
  
})

# time completion
Sys.time()



# # Plot
# all_region_surface_temp %>% 
#   mutate(time = as.Date(time)) %>% 
#   ggplot() +
#   geom_line(aes(time, regional_mu, color = area_id),
#             linewidth = 0.4, alpha = 0.4)


```



### Bottom Temperature Processing

```{r}
#| label: process bottom temperatures
#| eval: false


# Process Surface Temperature for all the regions:
fvcom_var_name <- "bottom_t"


# Time it
Sys.time()
all_region_bottom_temp <- map_dfr(fvcom_surfbot_files, function(nc_x){
  
  # 1. Open the netcdf:
  yr_x_fvcom <- nc_open(nc_x)
  
  # 2. Get the average values for the triangles
  # Run that for each of the years once
  triangle_avgs <- regional_from_nodes(
    fvcom_mesh_trios = lsag_area_intersections, 
    nc = yr_x_fvcom, 
    nc_varname = fvcom_var_name)
  
  
  # Get the regional averages for each region/group
  poly_avgs_yr_x <-  lsag_area_intersections %>% 
    split(.$area_id) %>% 
    map_dfr(
    .x = .,
    .f = ~fvcom_regional_averages(
      mesh_poly_intersection = .x,
      mesh_elem_avgs = triangle_avgs, 
      nc = yr_x_fvcom),
    .id = "area_id"
  )
  
  # Close the netcdf connection
  nc_close(yr_x_fvcom)
  
  # Return the regional averages
  return(poly_avgs_yr_x)
  
})

# time completion
Sys.time()



# Plot
all_region_bottom_temp %>% 
  mutate(time = as.Date(time)) %>% 
  ggplot() +
  geom_line(aes(time, regional_mu, color = area_id),
            linewidth = 0.4, alpha = 0.4)
```

### Combine Surface and Bottom and Save

```{r}
#| eval: false


# # Reshape/format them to put together
# # Put them into one table:
# regional_temperatures <- left_join(
#   all_region_surface_temp  %>% 
#     mutate(time = as.Date(time)) %>% 
#     rename(surface_t = regional_mu), 
#   all_region_bottom_temp  %>% 
#     mutate(time = as.Date(time)) %>% 
#     rename(bottom_t = regional_mu)) %>% 
#   mutate(area_id = str_split_i(area_id, "/", 2))
# 
# 
# # Plot to look
# regional_temperatures %>% 
#   ggplot() +
#   geom_line(aes(time, surface_t, color = "Surface Temperature"), linewidth = 0.8, alpha = 0.6) +
#   geom_line(aes(time, bottom_t, color = "Bottom Temperature"), linewidth = 0.8, alpha = 0.6) +
#   scale_color_gmri() +
#   facet_wrap(~area_id, nrow = 2)


# # # Exporting
# write_csv(
#   regional_temperatures,
#   here::here("local_data", "jkipp_regions_fvcom_temperatures_daily.csv"))


```




# Inspecting Timeseries

Prior to any interpolations, timeseries for both inshore and offshore areas are evaluated for both long-term trends (Kendall test), and structural breaks in means/trends (changepoint/breakpoint tests).

These tests determine the following:\
- Significant trends = non-stationary, and global krigging model should not be used\
- Breakpoints determine which groups of years should be used for seasonal kriging models

```{r}
#| label: both-version-inshoreoffshore



# # Load the collection if necessary:
# regional_temperatures <- read_csv(here::here("local_data", "jkipp_regions_fvcom_temperatures_daily.csv"))


# Load and add inshore/offshore labels on the daily data
regional_temperatures <- read_csv(
  str_c(lob_ecol_path, "FVCOM_processed/area_timeseries/jkipp_regions_fvcom_temperatures_daily.csv"))

# Plot to look
regional_temperatures %>%
  ggplot() +
  geom_line(aes(time, surface_t, color = "Surface Temperature"), linewidth = 0.8, alpha = 0.6) +
  geom_line(aes(time, bottom_t, color = "Bottom Temperature"), linewidth = 0.8, alpha = 0.6) +
  scale_color_gmri() +
  facet_wrap(~area_id, nrow = 2)

```



---




# Second Workflow: Triangle-Specific Timeseries

Jeff would like the daily data for August-December for the Gulf of Maine, but at the daily resolution.

It may make sense to send it to him as triangle averages, using their ID's, and a separate .geojson or .shp file that has the triangles themselves.




### Bottom Temperatures at Mesh Resolution


```{r}
# We can see all the triangles that are at play this way
lsag_area_intersections %>% filter(area_id == "OceanProductPoly_GOM") %>% distinct(elem)


```


```{r}
#| label: process element-wise bottom temperature timeseries
#| eval: false

# Process Surface Temperature for all the regions:
fvcom_var_name <- "bottom_t"


# Time it
Sys.time()

# Change the steps below, to loop over the three areas
all_element_bottom_temp <- fvcom_surfbot_files %>% 
  map_dfr(
    .x = ., 
    function(nc_x){
  
  # 1. Open the netcdf:
  yr_x_fvcom <- nc_open(nc_x)
  
  # 2. Get the average values for the triangles
  # Run that for each of the years once
  triangle_avgs <- regional_from_nodes(
    fvcom_mesh_trios = lsag_area_intersections,
    #fvcom_mesh_trios = filter(lsag_area_intersections, str_detect(area_id, "SNE")),
    nc = yr_x_fvcom,
    nc_varname = fvcom_var_name)
  
  # # Faster But is averaging over time too...
  # triangle_avgs <- average_from_nodes(
  #   fvcom_mesh_trios = lsag_area_intersections, 
  #   nc = yr_x_fvcom, 
  #   nc_varname = fvcom_var_name, 
  #   start_vec = c(1, 1), 
  #   count_vec = c(-1,-1))
   
  # Get the time dimension
  fv_dates <- data.frame(
    "time" = ncvar_get(yr_x_fvcom, "Times"))
  
  # Bind and filter the dates
  triangle_avgs <- bind_cols(fv_dates, triangle_avgs) %>% 
    # filter(month(time) >= 8)
    filter(month(time) == 7)
  
  # Close the netcdf connection
  nc_close(yr_x_fvcom)
  
  # Return the regional averages
  return(triangle_avgs)
  
})

# time completion
Sys.time()
```


```{r}
#| label: plot check of bottom temps

# REMEMBER, we downloaded specific months, so it will be jumpy

# Looks like we have daily now
all_element_bottom_temp %>% 
  mutate(time = as.Date(time)) %>% 
  ggplot() +
    geom_line(aes(time, `7567`))
```


```{r}

# Reshape
element_bottom_temps_long <- all_element_bottom_temp %>% 
  mutate(time = as.Date(time)) %>% 
  pivot_longer(cols = -time, names_to = "elem", values_to = "bottom_t") %>% 
  mutate(bottom_t = as.numeric(bottom_t)) 


# Join in the area labels
lsag_bottom_temps <- lsag_area_intersections %>% 
  distinct(area_id, elem) %>% 
    mutate(elem = as.character(elem)) %>% 
  inner_join(element_bottom_temps_long, join_by(elem), relationship = "many-to-many") 

# Plot a test day
day_test <- "2010-12-25"
day_test <- "2010-08-25"
day_test <- "2010-07-25"

# element_bottom_temps_long %>% 
lsag_bottom_temps %>% 
  filter(time == day_test) %>% 
  left_join(mutate(gom3_mesh, elem = as.character(elem))) %>% 
  st_as_sf() %>% 
  ggplot() +
  #geom_sf(data = new_england, color = "gray30") +
  geom_sf(aes(fill = bottom_t), color = "transparent") +
  facet_wrap(~area_id) +
  scale_fill_distiller(
    palette = "RdBu") +
  theme_dark() +
  theme(legend.position = "bottom", legend.title.position = "top", legend.title = element_text(hjust = 0.5)) +
  coord_sf(expand = F) +
  labs(title = str_c("FVCOM Daily Bottom Temperature:"),
       fill = "Bottom Temperature",
       subtitle = str_c("Date: ", day_test))



```




```{r}
#| eval: false
#| label: Save regional bottom temps

# # Daily Bottom temperature for all mesh areas in SNE
# rm(jkipp_original_gom, element_bottom_temps_long)
# gc()

# Save them by region
lsag_bottom_temps %>% 
  split(.$area_id) %>%
  iwalk(function(x, y){
    
    # # Try rds
    save_name <- here::here("local_data", str_c("dailyfvcomBT_", y, "_July.rds"))
   saveRDS(x, save_name)

  })
```


```{r}
# # test those rds
# sne_in <- readRDS(here::here("local_data", str_c("dailyfvcomBT_OceanProductPoly_SNEinshore_AugDec.rds")))
# sne_off <- readRDS(here::here("local_data", str_c("dailyfvcomBT_OceanProductPoly_SNEinshore_AugDec.rds")))
```



```{r}
#| eval: false
#| label: Save gom timeseries

# # This is the daily bottom temperature that Jeff wanted
# write_csv(
#   lsag_bottom_temps,
#   here::here("local_data/dailyfvcomBT_OceanProductPoly_GOM_AugDec.csv")
# )

# # Check it
# jkipp_original_gom <- read_csv(
# here::here("local_data/dailyfvcomBT_OceanProductPoly_GOM_AugDec.csv")
# )
```



# Extract Depth Information

Jeff asked if the bathymetry data was available, the next sections pull what the bottom bathymetry was and then whatever depth the sigma layer is taken to represent:

```{r}
# Find a GOM3 File with Depth
# Base URL path
gom3_base <- "http://www.smast.umassd.edu:8080/thredds/dodsC/models/fvcom/NECOFS/Archive/Seaplan_33_Hindcast_v1"


# year/month details
gom3_yr   <- 1978
gom3_mnth <- "01"
gom3_url  <- str_c(gom3_base, "/gom3_", gom3_yr, gom3_mnth, ".nc")


# # Open Connection
gom3_x <- nc_open(gom3_url)

# Pull bathymetry, figure out what its coordinates are
gom3_bathy <- ncvar_get(gom3_x, varid = "h", start = c(1))

# Pull the "bottom" siglay
surface_siglay <- ncvar_get(gom3_x, varid = "siglay", start = c(1, 1), count = c(-1,1))
bot_siglay <- ncvar_get(gom3_x, varid = "siglay", start = c(1, 45), count = c(-1,1))
range(surface_siglay)
range(bot_siglay)

# Bathymetry links to nodes*
# length(gom3_bathy) == gom3_x$dim$node$len # T
# length(gom3_bathy) == gom3_x$dim$nele$len # F
bathy_df <- data.frame(
  "lon" = ncvar_get(gom3_x, "lon"),
  "lat" = ncvar_get(gom3_x, "lat"),
  "bathy" = gom3_bathy,
  "surface_depth" = abs(surface_siglay) * gom3_bathy,
  "bottom_depth" = abs(bot_siglay) * gom3_bathy
)


# Plot the three layers
bathy_p <- ggplot(bathy_df, aes(lon, lat)) + 
  geom_point(aes(color = bathy), size = 0.2) +
  scale_color_distiller(
    palette = "Blues", direction = 1, 
    limits = c(0, max(gom3_bathy))) +
  theme_dark() +
  labs(title = "FVCOM Bathymetry (m)")
surface_p <- ggplot(bathy_df, aes(lon, lat)) + 
  geom_point(aes(color = surface_depth), size = 0.2) +
  scale_color_distiller(
    palette = "Blues", direction = 1, 
    limits = c(0, max(gom3_bathy))) +
  theme_dark() +
  labs(title = "Surface Layer Depth (m)")
bottom_p <-  ggplot(bathy_df, aes(lon, lat)) + 
  geom_point(aes(color = bottom_depth), size = 0.2) +
  scale_color_distiller(
    palette = "Blues", direction = 1, 
    limits = c(0, max(gom3_bathy))) +
  theme_dark() +
  labs(title = "Bottom Layer Depth (m)")

# Plot them all
bathy_p / (surface_p | bottom_p)

```




### Average Depth over Area

I'm using the new function from `Regional_Salinity_Monthly.qmd` to speed things up.

```{r}


#' @Title FVCOM Element Average from Vertices
#' 
#' 
#' @description Get the average value from the relevant vertices, along  
#' the time dimension for FVCOM mesh triangles.
#'
#' @param fvcom_mesh_trios
#' @param nc
#' @param nc_varname 
#' @param siglay index number for depth layer
#'
#' @return
#' @export
#'
#' @examples
average_from_nodes <- function(fvcom_mesh_trios, 
    nc, 
    nc_varname = "surface_t", 
    start_vec = c(),
    count_vec = c()){
  
  # Reduce any redundancies/repeats
  fvcom_mesh_trios <- distinct(fvcom_mesh_trios, elem, p1, p2, p3)
  
  # Pull the full slice of the variable for a single depth
  # Salinity coordinates are: node, siglay, time
  # Grab the trio that matches the triangle
  var_slice <- ncvar_get(
      nc, 
      varid = nc_varname, 
      start = start_vec, 
      count = count_vec)
  
  # Compute mean time series for each trio using purrr::map
  triangular_element_means <- pmap(
    .l = fvcom_mesh_trios, 
    .f = function(elem, p1, p2, p3) {
      p1_ts = var_slice[p1]
      p2_ts = var_slice[p2]
      p3_ts = var_slice[p3]
      trio_time_series <- (p1_ts + p2_ts + p3_ts) / 3
  })
  
  # Assign the triangle IDs to the list
  triangular_element_means <- setNames(triangular_element_means, fvcom_mesh_trios$elem)
  
  
  return(triangular_element_means)
  
}



```


```{r}
# Get the avg bathymetry
avg_depths <- average_from_nodes(
  fvcom_mesh_trios = lsag_area_intersections, 
  nc = gom3_x, 
  nc_varname = "h", 
  start_vec = c(1), 
  count_vec = c(-1))

# Get the surface depth
surf_siglay <- average_from_nodes(
  fvcom_mesh_trios = lsag_area_intersections, 
  nc = gom3_x, 
  nc_varname = "siglay", 
  start_vec = c(1,1), 
  count_vec = c(-1,1))

# Get the bootom depth
bot_siglay <- average_from_nodes(
  fvcom_mesh_trios = lsag_area_intersections, 
  nc = gom3_x, 
  nc_varname = "siglay", 
  start_vec = c(1,45), 
  count_vec = c(-1,1))

# Make it a dataframe
elem_depths <- map_dfr(
  .x = names(avg_depths), 
  .f = function(x){
    data.frame(
      "elem" = x,
      "bathy" = avg_depths[[x]],
      "surface_layer_depth" = abs(surf_siglay[[x]]) * avg_depths[[x]],
      "bottom_layer_depth" = abs(bot_siglay[[x]]) * avg_depths[[x]])})



# Join to mesh
bathy_mesh <- elem_depths %>%
  mutate(elem = as.numeric(elem) ) %>% 
  left_join(gom3_mesh, join_by("elem")) %>% 
  st_as_sf()

# Map it
bathy_mesh %>% 
  pivot_longer(cols = -c(elem, geometry, p1, p2, p3), names_to = "var", values_to = "depth") %>% 
  ggplot() +
  geom_sf(aes(fill = depth), color = NA) +
  facet_wrap(~var) +
  scale_fill_distiller(
    palette = "RdYlBu", direction = -1) +
  labs(title = "Bathymetry Depth (m)")


# Add the area labels back
bathy_mesh <- bathy_mesh %>% 
  left_join(
    lsag_area_intersections %>% distinct(area_id, elem)) %>% 
  relocate(area_id, .before = elem) %>% 
  st_as_sf()

```




```{r}
# Save the bathymetry with the mesh

# Get "bottom layer" depth?

# # this is the shapefile information
# sf::write_sf(bathy_mesh, here::here("local_data/OceanProductPoly_depths.geojson"))

# bathy_mesh_test <- st_read(here::here("local_data/OceanProductPoly_depths.geojson"))
# mesh_test <- st_read(here::here("local_data/FVCOM_gom3_mesh.geojson"))
# ggplot(mesh_test) +geom_sf()
```




# Nodes/Elements Resolution

As another layer of information, it may be useful to know how many nodes/elements (proxies for resolution) contributed to mean estimates:

```{r}

# This gets the average area of the polygons within the strata
# Be careful with the units here! relative area is in meters, units come along for the ride
area_resolutions <- lob_areas_intersections %>% 
  group_by(area_id = lob_area) %>% 
  summarise(n_elems = n_distinct(elem),
            avg_area_km2 =  as.numeric(mean(rel_area)/1e6))

# We also probably want the number of nodes, and center elements
# we can get the center coordinates this way
gom3_mesh_elems <- data.frame(
  "lonc" = ncvar_get(fvcom_yrx, "lonc"),
  "latc" = ncvar_get(fvcom_yrx, "latc")) %>% 
  distinct(lonc, latc) %>% 
  mutate(elem = row_number()) %>% 
  st_as_sf(coords = c("lonc", "latc"), 
           crs = st_crs(gom3_mesh), 
           remove = F)
# and the nodes this way
gom3_mesh_nodes <- data.frame(
  "lon" = ncvar_get(fvcom_yrx, "lon"),
  "lat" = ncvar_get(fvcom_yrx, "lat")) %>% 
  mutate(node = row_number()) %>% 
  st_as_sf(coords = c("lon", "lat"), 
           crs = st_crs(gom3_mesh), 
           remove = F)

# validate we have the node/center coordinates right
ggplot() +
  geom_sf(data = gom3_mesh_nodes, shape = 3, size = 0.4, alpha = 0.6, color = "#ebcb27" ) +
  geom_sf(data = gom3_mesh_elems, shape = 3, size = 0.4, alpha = 0.6, color = "#07a3b7") + 
  theme_dark() +
  theme(panel.grid = element_blank()) +
  # coord_sf(xlim = c(-71.5, -69.5), ylim = c(41, 42.5)) # Cape Cod
  #coord_sf(xlim = c(-70.5, -68), ylim = c(43.5, 44.25)) # Maine
  coord_sf(xlim = c(-70.35, -67.25), ylim = c(43.25, 44.65)) # Maine


# Use crop to count the points within each one
# Or use intersection?
area_points <- map_dfr(
  .x = lob_areas_trans, 
  .f = possibly(function(strata_x){
  n_nodes <- st_intersection(x = gom3_mesh_nodes, y = strata_x) %>% 
  #n_nodes <- st_crop(x = gom3_mesh_nodes, y = strata_x) %>% 
      nrow()
  n_centers <- st_intersection(x = gom3_mesh_elems, y = strata_x) %>% 
  #n_centers <- st_crop(x = gom3_mesh_elems, y = strata_x) %>% 
      nrow()
  tibble(
    "n_nodes" = n_nodes,
    "n_centers" = n_centers)
  }, 
  otherwise = tibble("n_nodes" = 0, "n_centers" = 0)), 
  .id = "area_id")



# totally missed it, use the new trick i learned
#strata_points <- .Last.value




# Join them together:
area_resolution_data <- left_join(
  mutate(area_resolutions, area_id = as.character(area_id)),
  y = area_points) %>% 
  mutate(area_id = str_split_i(area_id, "/", 2))



# Check the distribution
#hist(strata_resolution_data$avg_area_km2)

# Mqp the avg resolution in each strata
bind_rows(lob_areas_trans, .id = "area_id") %>% 
  mutate(area_id = str_split_i(area_id, "/", 2)) %>% 
  left_join(area_resolution_data) %>% 
  mutate(avg_area_km2 = as.numeric(avg_area_km2)) %>% 
  ggplot() +
  geom_sf(
    data = gom3_mesh_inside, 
    color = "gray60", alpha = 0.2, linewidth = 0.1) +
  geom_sf(aes(fill = avg_area_km2), alpha = 0.3) +
  scale_fill_distiller(palette = "RdYlGn")



# # Save that out:
# write_csv(area_resolution_data, here::here("local_data", "jkipp_regions_fvcom_elem_resolution.csv"))

# # check it
# read_csv(here::here("local_data", "jkipp_regions_fvcom_elem_resolution.csv"))
```


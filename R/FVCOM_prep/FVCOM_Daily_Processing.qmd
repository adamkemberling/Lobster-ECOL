---
title: "Handling Daily/Hourly FVCOM Data"
format: html
description: | 
  Approaches for handling FVCOM's Densest Files
date: "Updated on: `r Sys.Date()`"
format: 
  html:
    code-fold: true
    code-tools: true
    df-print: kable
    self-contained: true
execute: 
  echo: true
  warning: false
  message: false
  fig.align: "center"
  comment: ""
---

# Handling Daily/Hourly FVCOM Data (NECOFS 2016-2023)

FVCOM data is large, and the formats with daily/hourly formats are stored remotely with limited access bandwidth.

We need to access daily FVCOM information for two purposes:\*
 1. To perform date/time/location matching to point locations of research survey
 2. Calculate metrics related to heat stress and time within thermal preference ranges for species

```{r}
#library(raster)
library(sf) 
library(fvcom) 
library(ncdf4) 
library(tidyverse)
library(gmRi)
library(patchwork)

conflicted::conflict_prefer("select", "dplyr")
conflicted::conflict_prefer("filter", "dplyr")
proj_path <- cs_path("mills", "Projects/Lobster ECOL")
fvcom_out <- str_c(proj_path, "FVCOM_support/")

source(here::here("R/FVCOM_Support.R"))
```

# GOM3 - Prototyping

### Accessing Monthly Data via THREDDS/OPENDAP Access

For the years 1978-2016 we can access hindcast data for FVCOM-GOM3, which contains all variables. After 2016 we need to transition to a newer grid version. **Monthly averaged data is available through 2020, but only for temperature.**

```{r}
#| label: Hindcast-data-access

# We can Access links from the THREDDS directory as if they are NETCDF files
hcast_base <- "http://www.smast.umassd.edu:8080/thredds/dodsC/models/fvcom/NECOFS/Archive/Seaplan_33_Hindcast_v1/"

# Files are monthly, so pick a month that year
hcast_yr <- "2016"
hcast_mon <- "07"
hcast_url <- str_c(hcast_base, "gom3_", hcast_yr, hcast_mon, ".nc")

# Open Connection
hcast_x <- nc_open(hcast_url)

hcast_x$dim
hcast_x$var[3]

# # Hourly timestep
# fvcom_time(hcast_x)

```



## Computing Monthly Mean Product - ignoring hours

To get to a monthly average downloading as little data as possible we can do a cumulative average process where we loop through each timestep and add on to the previous sum. At the end of the month, we divide by the number of days in the month to get an average.

Done this way the size of the data in memory is only as big as the end amount, which whould be ~100MB. the bottleneck will be how long it takes to download specific variables for specific depths for all the hours in a month.

The code below shows what that looks like, using the hindcast data to speed up the processing time (less time steps).

```{r}
#| label: accessing-hours-at-a-time
#| eval: false

# # Number of hours:
# num_hours <- fvcom_time(hcast_x)
# 
# # How many sigma layers (depth)
# max_depth <- dim(ncvar_get(hcast_x, "siglay"))[2]
# 
# # static elements (grab once) - quick
# lon_vals <- ncvar_get(hcast_x, "lon")
# lat_vals <- ncvar_get(hcast_x, "lat")
# 
# 
# # Need to loop through this structure, and it will spit out values for all the depths
# t_index <- 1
# 
# # Can't open multiple timesteps at once to slice away into the depth dimension
# # Request too big=6497.0 Mbytes, max=1000.0
# 
# # Will need to open each hour alone, then index out the surface and bottom
# # This is surface temp and bottom temp at that one time step
# stemp_1 <- ncvar_get(hcast_x, "temp", start = c(1, 1, t_index), count = c(-1,-1,1))[,1]
# btemp_1 <- ncvar_get(hcast_x, "temp", start = c(1, 1, t_index), count = c(-1,-1,1))[,max_depth]
# 
# # This is the next time step, just increment on the time index
# stemp_2 <- ncvar_get(hcast_x, "temp", start = c(1, 1, t_index+1), count = c(-1,-1,1))[,1]
```


```{r}
#| label: processing-one-month-temps-gom3
#| eval: false
#| echo:true

# # ----- Build File URL
# 
# # Files are monthly, so pick a month that year
# hcast_yr <- "2016"
# hcast_mon <- "07"
# hcast_url <- str_c(hcast_base, "gom3_", hcast_yr, hcast_mon, ".nc")
# 
# 
# #---- Open connection - get static elements
# 
# 
# # Open Connection
# hcast_x <- nc_open(hcast_url)
# 
# # Number of hours:
# num_hours <- fvcom_time(hcast_x)
# 
# # How many sigma layers (depth)
# max_depth <- dim(ncvar_get(hcast_x, "siglay"))[2]
# 
# # Get static elements
# #  (grab once) - quick
# lon_vals <- ncvar_get(hcast_x, "lon")
# lat_vals <- ncvar_get(hcast_x, "lat")
# 
# 
# #------- loop through time steps
# 
# # # placeholder vectors to hold the data as it gets looped through
# stemp <- rep(0, length(lon_vals))
# btemp <- rep(0, length(lon_vals))
# 
# # Test the time it takes
# Sys.time()
# for (x_time in seq(1, length(num_hours))) {
#   
#   # Get the new hours data
#   new_stemp <- ncvar_get(hcast_x, "temp", start = c(1, 1, x_time), count = c(-1,-1,1))[,1]
#   new_btemp <- ncvar_get(hcast_x, "temp", start = c(1, 1, x_time), count = c(-1,-1,1))[,max_depth]
#   
#   # Add it to the running total
#   stemp <- stemp + new_stemp
#   btemp <- btemp + new_btemp
#   
#   rm(new_stemp, new_btemp)
#   gc(verbose = F)
# }
# Sys.time()
# 
# 
# #------ Divide by total steps
# 
# # divide by the number of timesteps:
# avg_stemp <- stemp/length(num_hours)
# avg_btemp <- btemp/length(num_hours)
# 
# 
# 
# #------  assemble df, save it
# 
# # Save it
# gom3_month_summ <- data.frame(
#   lon = lon_vals,
#   lat = lat_vals,
#   stemp = avg_stemp,
#   btemp = avg_btemp,
#   date = "201707",
#   mod = "FVCOM-GOM3")
# 
# # # Save it - using month info
# # write_csv(
# #   gom3_month_summ, 
# #   here::here("data", str_c("/gom3_monthly_", hcast_yr, "-", hcast_mon,"_test.csv")))
```


```{r}
#| label: verify-gom3-data
#| eval: true

# read one of the monthly summaries
gom3_month_summ <- read_csv(here::here("data", str_c("/gom3_monthly_", hcast_yr, "-", hcast_mon,"_test.csv")))

library(rnaturalearth)
new_england <- ne_states("united states of america", returnclass = "sf")
canada <- ne_states("canada", returnclass = "sf")



# Plot them to verify
gom3_month_summ %>% 
  pivot_longer(cols = ends_with("temp"), names_to = "var", values_to = "temperature") %>% 
  ggplot() +
  geom_point(aes(lon, lat, color = temperature), size = 0.25) +
  geom_sf(data = new_england) +
  geom_sf(data = canada) +
  theme_bw() + map_theme() +
  scale_color_distiller(palette = "RdBu") +
  facet_wrap(~var, ncol = 1) +
  coord_sf(xlim = c(-76, -57),ylim = c(35, 46)) +
  labs(title = str_c(hcast_yr, "-", hcast_mon, " FVCOM-GOM3"))
```


# NECOFS-GOM4



### NECOFS Hourly Product Processing - daily from hourly

For the years 2017-2023 we can use NECOFS, which uses the GOM4 mesh and boundary conditions.


```{r}
#| label: NECOFS-data-access

# We can Access links from the THREDDS directory as if they are NETCDF files


# Files contain data for one month, 
# so pick a month, year to construct a THREDDS connection url

# Base URL path
necofs_base <- "http://www.smast.umassd.edu:8080/thredds/dodsC/models/fvcom/NECOFS/Archive/NECOFS_GOM/"

# year/month details
necofs_yr   <- 2017
necofs_mnth <- "01"
necofs_url  <- str_c(necofs_base, necofs_yr,  "/gom4_", necofs_yr, necofs_mnth, ".nc")


# Open Connection
necofs_x <- nc_open(necofs_url)
```


```{r}
#| label: necofs-static-elements

#------- Get static elements
#  (grab once) - quick
# Location indices and coordinate dimensions don't change over time
# We can grab those one time and store them

# Get the max sigma layer (depth)
max_depth <- dim(ncvar_get(necofs_x, "siglay"))[2]

# Lat & Lon
lon_vals <- ncvar_get(necofs_x, "lon")
lat_vals <- ncvar_get(necofs_x, "lat")
```


```{r}
#| label: necofs-time-dimension-details


#-------- Get Time Vector information

# Date/Time values:
necofs_times  <- fvcom_time(necofs_x)
necofs_dates  <- lubridate::date(necofs_times)
necofs_dayset <- lubridate::day(necofs_times)
necofs_days   <- unique(necofs_dayset)


```


```{r}
#| label: necofs-single-day-test
#| eval: false

# full_day will be loop index
full_day <- necofs_days[1]

# Get the indices for start/count that match that full_day
use_times <- which(necofs_dayset == full_day)
use_start <- min(use_times)
use_count <- length(use_times)


# Get the data for that day:
# Gonna have to repeat for each hour


# Pull Surface Layer for each timestep within the day
new_stemps <- map(
  use_times,
  function(use_start){
    ncvar_get(
      nc = necofs_x, 
      varid = "temp", 
      start = c(1, 1, use_start), 
      count = c(-1,-1, 1))[,1]
  }
)


# Now get the value across those time steps
stemp <- rep(0, length(lon_vals))
for(hr in seq(1, length(new_stemps))) {
  stemp <- stemp + new_stemps[[hr]]
}


# Divide that by num hours to get mean
new_daily_stemp <- stemp / length(new_stemps)



#------  assemble df at the end to include lat/lon/date, save it

# Save it
gom4_tday_summ <- data.frame(
  lon   = lon_vals,
  lat   = lat_vals,
  stemp = new_daily_stemp,
  date  = str_c(necofs_yr, necofs_mnth, "0", necofs_days[1], sep = "-"),
  mod   = "NECOFS-GOM4")

ggplot(gom4_tday_summ) +
  geom_point(aes(lon, lat, color = stemp), size = 0.25) +
  scale_color_distiller(palette= "RdBu") +
  labs(title = "Single Day Average Test",
       subtitle = str_c(necofs_yr, necofs_mnth, "0", necofs_days[1], sep = "-"))
```

# Daily Averages for Full Month

Monthly files containing hourly (or multi-hour) timesteps can be converted to daily averages this way:


```{r}
# Looping through each day:


#' @title Process Daily NECOFS Surface & Bottom
#'
#' @param fvcom_nc ncdf object for the monthly file to process
#' @param date_vector day_number_vector, named as yyyy-mm-dd full dates for labeling
#' @param timestep_dom time step vector indicating which day of the month each timestep belongs
#' @param var_id nc variable name ("temp") in this case
#' @param bot_siglev Bottom layer sigma level (siglev)
#' @param total_nodes length of lon or lat dimension to indicate vector length for making an empty vector 
#'
#' @return
#' @export
#'
#' @examples
process_daily_surf_and_bot <- function(
    fvcom_nc,               
    var_id = "temp"){
  
  
  # 1. 
  # Extract Static Elements:
  necofs_times  <- fvcom_time(fvcom_nc) # Actual times from nc
  necofs_dates  <- lubridate::date(necofs_times) # Just the date component
  timestep_dom  <- lubridate::day(necofs_times) # The day of month for the dates
  necofs_days   <- unique(timestep_dom) # The unique day of month
  date_vector   <- setNames(necofs_days, unique(necofs_dates))
  
  # Lat & Lon
  lon_vals <- ncvar_get(fvcom_nc, "lon")
  lat_vals <- ncvar_get(fvcom_nc, "lat")
  
  # Bottom level index and total nodes
  bot_siglev  <- dim(ncvar_get(fvcom_nc, "siglay"))[2]
  total_nodes <- length(lon_vals)


  
  # 2.
  # Use map to process daily means
  daily_avgs_df <- map_dfr(
    # Operating on day of month, and the full date for it
    date_vector, 
    function(full_day_x){
    
    # Get the indices for start/count that match that full_day
    use_times <- which(timestep_dom == full_day_x)
    use_start <- min(use_times)
    
    # Get the data for that day, pulling data for each hour
    # Surface Layer
    hrly_surface <- map(
      use_times,
      function(use_start){
        ncvar_get(
          nc = fvcom_nc, 
          varid = var_id, 
          start = c(1, 1, use_start), 
          count = c(-1,-1, 1))[,1]})
      
      # Now get the average across those time steps
      daily_surface <- rep(0, total_nodes)
      for(hr in seq(1, length(hrly_surface))) {
        daily_surface <- daily_surface + hrly_surface[[hr]]
      }
      
      # divide that by num hours - get daily average
      daily_surf_avg <- daily_surface / length(hrly_surface)
      
      # ------- Repeat for bottom layer ------
      
      # Bottom layer
      hrly_bottom <- map(
          use_times,
          function(use_start){
            ncvar_get(
              nc = fvcom_nc, 
              varid = var_id, 
              start = c(1, 1, use_start), 
              count = c(-1,-1, 1))[, bot_siglev]})
    
      # Now get the average across those time steps
      daily_bottom <- rep(0, total_nodes)
      for(hr in seq(1, length(hrly_bottom))) {
        daily_bottom <- daily_bottom + hrly_bottom[[hr]]
      }
      
      # divide by num hours - get daily average
      daily_bot_avg <- daily_bottom / length(hrly_bottom)
  
    
    # Tidy up
    rm(daily_bottom, hrly_bottom, daily_surface, hrly_surface)
    gc(verbose = F)
    
    
    # Return it as a dataframe
    names_out <- c("lon", "lat", str_c("surf_", var_id), str_c("bot_", var_id))
    daily_out <- data.frame(
      lon      = lon_vals,
      lat      = lat_vals,
      surf_var = daily_surf_avg, 
      bot_var  = daily_bot_avg) %>% 
      setNames(names_out)
    
    return(daily_out)
  
    
    }, .id ="date")
  
}
```


```{r}
#| label: month-function-test

# Perform Daily Processing for one month
# Can put these in a big list if we want to do multiple
# then save as we go when doing that


# Base URL path
necofs_base <- "http://www.smast.umassd.edu:8080/thredds/dodsC/models/fvcom/NECOFS/Archive/NECOFS_GOM/"
  
# year/month details
necofs_yr   <- 2017
necofs_mnth <- "01"
necofs_url  <- str_c(necofs_base, necofs_yr,  "/gom4_", necofs_yr, necofs_mnth, ".nc")

# Open Connection
necofs_x <- nc_open(necofs_url)
  
  
# Check the time before
Sys.time()

jan17_daily <- process_daily_surf_and_bot(
    fvcom_nc = necofs_x,               
    var_id = "temp")

# Check the time
Sys.time()



# Plot it
jan17_daily %>% 
  filter(date %in% c("2017-01-01", "2017-01-15")) %>% 
  pivot_longer(cols = c(bot_temp, surf_temp), names_to = "var", values_to = "temp") %>% 
  ggplot() +
  geom_point(aes(lon, lat, color = temp), size = 0.2) +
  scale_color_distiller(palette = "RdBu") +
  facet_grid(date~var)



# # Save it - using month info
# write_csv(
#   gom3_month_summ,
#   here::here("data", str_c("/gom3_monthly_", hcast_yr, "-", hcast_mon,"_test.csv")))

```


# Process a Year

```{r}



# # Saving two months as tester:
# process_dates <- list(
#   "years" = rep("2017", 2),
#   "months" = str_pad(c(1:2), width = 2, side = "left", pad = "0")
# )

# Saving a year:
process_dates <- list(
  "years" = rep("2017", 12),
  "months" = str_pad(c(1:12), width = 2, side = "left", pad = "0")
)

#### Global constants

# Save folder
save_location <- cs_path("mills", "Projects/Lobster ECOL/FVCOM_processed/NECOFS_daily")

# Base URL path
necofs_base <- "http://www.smast.umassd.edu:8080/thredds/dodsC/models/fvcom/NECOFS/Archive/NECOFS_GOM/"


####  The big ole processin loop
map2(
  .x = process_dates$years,
  .y = process_dates$months,
  function(yr_x = .x, mon_x = .y){
    
    # THREDDS URL for the month+year
    month_url  <- str_c(necofs_base, yr_x,  "/gom4_", yr_x, mon_x, ".nc")
      
    # Open the connection
    necofs_month_x <- nc_open(month_url)
    
    # Process Daily
    daily_mean_dat <- process_daily_surf_and_bot(
      fvcom_nc = necofs_month_x,               
      var_id = "temp")
    
    # Save the data
    save_name <- str_c(save_location, "NECOFS_daily_surfbottemp_", yr_x, "_", mon_x, ".csv")
    write_csv(
      daily_mean_dat,
      save_name)
    
    
    # Close the connection
    nc_close(necofs_month_x)
    
    # Garbage collection
    rm(necofs_month_x, daily_mean_dat)
    gc(verbose = F)
  }
)

```



## Monthly Data from Hourly NECOFS Data

NECOFS does not come pre-packaged as monthly files, so to get a monthly record it requires building it up from the hourly data.

If we follow the code from before we can process monthly timeseries for the NECOFS FVCOM coverage through 2023.

```{r}
#| label: processing-one-month-necofs
#| eval: false
#| echo: true

# Number of hours:
num_hours <- fvcom_time(necofs_x)

# How many sigma layers (depth)
max_depth <- dim(ncvar_get(necofs_x, "siglay"))[2]

# static elements (grab once) - quick
lon_vals <- ncvar_get(necofs_x, "lon")
lat_vals <- ncvar_get(necofs_x, "lat")

# # placeholder vectors to hold the data as it gets looped through
stemp <- rep(0, length(lon_vals))
btemp <- rep(0, length(lon_vals))


# Test the time it takes to grab one hour using a rolling average approach


# garbage collect after each day's worth of hours
# https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/releasememory.html
gc_after_a_number <- 24

Sys.time()
for (x_time in seq(1, length(num_hours))) {
  # Get the new hours data
  new_stemp <- ncvar_get(necofs_x, "temp", start = c(1, 1, x_time), count = c(-1,-1,1))[,1]
  new_btemp <- ncvar_get(necofs_x, "temp", start = c(1, 1, x_time), count = c(-1,-1,1))[,max_depth]
  
  # Add it to the running total
  stemp <- stemp + new_stemp
  btemp <- btemp + new_btemp
  
  # clean up memory
  rm(new_stemp, new_btemp)
  if(x_time %% gc_after_a_number == 0){
    gc()
  }
  
}
Sys.time()

# divide by the number of timesteps:
avg_stemp <- stemp/length(num_hours)
avg_btemp <- btemp/length(num_hours)

# Save it
necofs_month_summ <- data.frame(
  lon = lon_vals,
  lat = lat_vals,
  stemp = avg_stemp,
  btemp = avg_btemp,
  date = "201707",
  mod = "NECOFS-GOM4")

# Save it
write_csv(necofs_month_summ, here::here("data/necofs_monthly_test.csv"))
```



```{r}
necofs_month_summ <- read_csv(here::here("data/necofs_monthly_test.csv"))

# Plot them
necofs_month_summ %>% 
  ggplot(aes(lon, lat, color = stemp)) +
  geom_point(size = 0.4) +
  theme_dark() +
  scale_color_distiller(palette = "RdBu") +
  labs(title = "Average Surface Temps for 2017-07, NECOFS FVCOM-GOM4",
       subtitle = "Takes ~30min to create surface+bottom monthly average")
```



### Building a Mesh from Node Values

If we plan to overlay points onto the mesh and get an interpolated value we can achieve this by either re-gridding to a standard grid, or by building a mesh with polygon center values similar to the FVCOM r package.

```{r}

```








## Matching Date/Time Locations to Daily Records

This is relatively straightforward for dates from 1978-2016. It becomes more cumbersome after 2016 because we'll need to get a daily average using monthly records.

What we can do to lesson the amount of data being requested is use lat/lon matching to determine the nearest nodes that we need. Then we can load data for just that lat/lon node using its index number.

Otherwise we need to load a region near it, which is really inefficient since the grid isn't ordered. That will force us to pull the whole region in for every hour that day, subsetting to a smaller area each time once its already loaded...


```{r}
# Load some lat/lon/time coordinate info


# Make
```

